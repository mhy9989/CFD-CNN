2024-03-15 11:12:22,947 - Use distributed mode with GPUs, world_size: 1
2024-03-15 11:12:22,947 - The base criterion is created. Base criterion type: MSE
2024-03-15 11:12:24,432 - The neural network is created. Network type: sau
2024-03-15 11:12:24,433 - The optimizer is created. Optimizer type: Adamw
2024-03-15 11:12:24,433 - The scheduler is created. Scheduler type: onecycle
2024-03-15 11:12:24,442 - Added key: store_based_barrier_key:2 to store for rank: 0
2024-03-15 11:12:24,442 - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.
2024-03-15 11:12:37,754 - Length of all dataset: 990
2024-03-15 11:12:37,755 - Length of train_dataset: 940
2024-03-15 11:12:37,755 - Length of valid_dataset: 0
2024-03-15 11:12:37,755 - Length of test_dataset: 50
2024-03-15 11:12:37,756 - Shape of input_data: torch.Size([10, 1, 128, 128])
2024-03-15 11:12:37,756 - Shape of label_data: torch.Size([1, 1, 128, 128])
2024-03-15 11:12:37,758 - 
seed: 	199989	
diff_seed: 	False	
per_device_train_batch_size: 	4	
per_device_valid_batch_size: 	4	
gradient_accumulation_steps: 	1	
num_workers: 	12	
method: 	sau	
max_epoch: 	500	
lossfun: 	MSE	
load_from: 	False	
if_continue: 	True	
regularization: 	0.0	
if_display_method_info: 	True	
mem_log: 	False	
empty_cache: 	False	
metrics: 	['MSE', 'RMSE', 'MAE', 'MRE', 'SSIM', 'MXRE']	
fps: 	True	
drop_last: 	False	
data_path: 	../../cfd-data-1001/CFD_data.npy	
mesh_path: 	../../cfd-data-1001/CFD_mesh.npy	
data_num: 	1000	
data_type: 	['u', 'v', 'T', 'P']	
data_select: 	[2]	
data_width: 	1181	
data_height: 	220	
data_range: 	[[0, 128], [0, 128]]	
data_mean: 	[0.7160895679671404, 0.10879711642281557, 3.15980087314138, 6.678367111021137]	
data_std: 	[0.34781700305722757, 0.17350806486928866, 2.2907320741672956, 8.661780748427242]	
data_max: 	[1.4213101, 1.08254902, 12.95459985, 78.43348863]	
data_min: 	[-0.55474735, -0.68369049, 0.77961313, 0.24014292]	
data_scaler: 	Standard	
data_previous: 	10	
data_after: 	1	
valid_ratio: 	0	
text_num: 	50	
optim: 	Adamw	
lr: 	0.0001	
filter_bias_and_bn: 	False	
log_step: 	1	
opt_eps: 	None	
opt_betas: 	None	
momentum: 	0.9	
weight_decay: 	0.01	
early_stop_epoch: 	-1	
sched: 	onecycle	
decay_rate: 	0.1	
decay_epoch: 	100	
lr_k_decay: 	1.0	
final_div_factor: 	10000.0	
min_lr: 	1e-06	
warmup_lr: 	1e-05	
warmup_epoch: 	0	
model_type: 	sau	
hid_S: 	32	
hid_T: 	256	
N_S: 	2	
N_T: 	8	
spatio_kernel_enc: 	3	
spatio_kernel_dec: 	3	
drop_path: 	0.1	
alpha: 	0.1	
beta: 	0.3	
offload: 	False	
zero_stage: 	0	
gradient_clipping: 	2.0	
total_length: 	11	
data_type_num: 	4	
data_select_num: 	1	
local_rank: 	0	
data_shape: 	[4, 220, 1181]	
in_shape: 	[10, 1, 128, 128]	
out_shape: 	[1, 1, 128, 128]	
data_use: 	['T']	
device: 	cuda:0	
rank: 	0	
world_size: 	1	
dist: 	True	
batch_size: 	4	
steps_per_epoch: 	248	
by_epoch: 	False	
2024-03-15 11:13:42,901 - Model info:
DeepSpeedEngine(
  (module): SimVP_Model(
    (enc): Encoder(
      (enc): Sequential(
        (0): ConvSC(
          (conv): BasicConv2d(
            (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm): GroupNorm(2, 32, eps=1e-05, affine=True)
            (act): SiLU()
          )
        )
        (1): ConvSC(
          (conv): BasicConv2d(
            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (norm): GroupNorm(2, 32, eps=1e-05, affine=True)
            (act): SiLU()
          )
        )
      )
    )
    (dec): Decoder(
      (dec): Sequential(
        (0): ConvSC(
          (conv): BasicConv2d(
            (conv): Sequential(
              (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): PixelShuffle(upscale_factor=2)
            )
            (norm): GroupNorm(2, 32, eps=1e-05, affine=True)
            (act): SiLU()
          )
        )
        (1): ConvSC(
          (conv): BasicConv2d(
            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm): GroupNorm(2, 32, eps=1e-05, affine=True)
            (act): SiLU()
          )
        )
      )
      (readout): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (hid): MidMetaNet(
      (enc): Sequential(
        (0): MetaBlock(
          (block): TAUSubBlock(
            (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (attn): TemporalAttention(
              (proj_1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
              (activation): GELU(approximate='none')
              (spatial_gating_unit): SpatiotemporalAttentionModule(
                (conv0): Conv2d(320, 320, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=320)
                (conv_spatial): Conv2d(320, 320, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=320)
                (conv1): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
                (pool_h): AdaptiveAvgPool2d(output_size=(None, 1))
                (pool_w): AdaptiveAvgPool2d(output_size=(1, None))
                (conv_c): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
                (pool): AdaptiveAvgPool1d(output_size=1)
                (sigmoid): Sigmoid()
              )
              (proj_2): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop_path): DropPath(drop_prob=0.010)
            (norm2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (mlp): MixMlp(
              (fc1): Conv2d(320, 2560, kernel_size=(1, 1), stride=(1, 1))
              (dwconv): DWConv(
                (dwconv): Conv2d(2560, 2560, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2560)
              )
              (act): GELU(approximate='none')
              (fc2): Conv2d(2560, 320, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (reduction): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): MetaBlock(
          (block): TAUSubBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (attn): TemporalAttention(
              (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
              (activation): GELU(approximate='none')
              (spatial_gating_unit): SpatiotemporalAttentionModule(
                (conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
                (conv_spatial): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=256)
                (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
                (pool_h): AdaptiveAvgPool2d(output_size=(None, 1))
                (pool_w): AdaptiveAvgPool2d(output_size=(1, None))
                (conv_c): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
                (pool): AdaptiveAvgPool1d(output_size=1)
                (sigmoid): Sigmoid()
              )
              (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop_path): DropPath(drop_prob=0.023)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (mlp): MixMlp(
              (fc1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))
              (dwconv): DWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): MetaBlock(
          (block): TAUSubBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (attn): TemporalAttention(
              (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
              (activation): GELU(approximate='none')
              (spatial_gating_unit): SpatiotemporalAttentionModule(
                (conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
                (conv_spatial): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=256)
                (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
                (pool_h): AdaptiveAvgPool2d(output_size=(None, 1))
                (pool_w): AdaptiveAvgPool2d(output_size=(1, None))
                (conv_c): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
                (pool): AdaptiveAvgPool1d(output_size=1)
                (sigmoid): Sigmoid()
              )
              (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop_path): DropPath(drop_prob=0.036)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (mlp): MixMlp(
              (fc1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))
              (dwconv): DWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): MetaBlock(
          (block): TAUSubBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (attn): TemporalAttention(
              (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
              (activation): GELU(approximate='none')
              (spatial_gating_unit): SpatiotemporalAttentionModule(
                (conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
                (conv_spatial): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=256)
                (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
                (pool_h): AdaptiveAvgPool2d(output_size=(None, 1))
                (pool_w): AdaptiveAvgPool2d(output_size=(1, None))
                (conv_c): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
                (pool): AdaptiveAvgPool1d(output_size=1)
                (sigmoid): Sigmoid()
              )
              (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop_path): DropPath(drop_prob=0.049)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (mlp): MixMlp(
              (fc1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))
              (dwconv): DWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (4): MetaBlock(
          (block): TAUSubBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (attn): TemporalAttention(
              (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
              (activation): GELU(approximate='none')
              (spatial_gating_unit): SpatiotemporalAttentionModule(
                (conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
                (conv_spatial): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=256)
                (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
                (pool_h): AdaptiveAvgPool2d(output_size=(None, 1))
                (pool_w): AdaptiveAvgPool2d(output_size=(1, None))
                (conv_c): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
                (pool): AdaptiveAvgPool1d(output_size=1)
                (sigmoid): Sigmoid()
              )
              (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop_path): DropPath(drop_prob=0.061)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (mlp): MixMlp(
              (fc1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))
              (dwconv): DWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (5): MetaBlock(
          (block): TAUSubBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (attn): TemporalAttention(
              (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
              (activation): GELU(approximate='none')
              (spatial_gating_unit): SpatiotemporalAttentionModule(
                (conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
                (conv_spatial): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=256)
                (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
                (pool_h): AdaptiveAvgPool2d(output_size=(None, 1))
                (pool_w): AdaptiveAvgPool2d(output_size=(1, None))
                (conv_c): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
                (pool): AdaptiveAvgPool1d(output_size=1)
                (sigmoid): Sigmoid()
              )
              (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop_path): DropPath(drop_prob=0.074)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (mlp): MixMlp(
              (fc1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))
              (dwconv): DWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (6): MetaBlock(
          (block): TAUSubBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (attn): TemporalAttention(
              (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
              (activation): GELU(approximate='none')
              (spatial_gating_unit): SpatiotemporalAttentionModule(
                (conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
                (conv_spatial): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=256)
                (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
                (pool_h): AdaptiveAvgPool2d(output_size=(None, 1))
                (pool_w): AdaptiveAvgPool2d(output_size=(1, None))
                (conv_c): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
                (pool): AdaptiveAvgPool1d(output_size=1)
                (sigmoid): Sigmoid()
              )
              (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop_path): DropPath(drop_prob=0.087)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (mlp): MixMlp(
              (fc1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))
              (dwconv): DWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (7): MetaBlock(
          (block): TAUSubBlock(
            (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (attn): TemporalAttention(
              (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
              (activation): GELU(approximate='none')
              (spatial_gating_unit): SpatiotemporalAttentionModule(
                (conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
                (conv_spatial): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=256)
                (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
                (pool_h): AdaptiveAvgPool2d(output_size=(None, 1))
                (pool_w): AdaptiveAvgPool2d(output_size=(1, None))
                (conv_c): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
                (pool): AdaptiveAvgPool1d(output_size=1)
                (sigmoid): Sigmoid()
              )
              (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (drop_path): DropPath(drop_prob=0.100)
            (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (mlp): MixMlp(
              (fc1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))
              (dwconv): DWConv(
                (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
              )
              (act): GELU(approximate='none')
              (fc2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (reduction): Conv2d(256, 320, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
)
| module                           | #parameters or shape   | #flops     |
|:---------------------------------|:-----------------------|:-----------|
| module                           | 11.905M                | 49.365G    |
|  enc.enc                         |  9.696K                |  0.457G    |
|   enc.enc.0.conv                 |   0.384K               |   73.4M    |
|    enc.enc.0.conv.conv           |    0.32K               |    47.186M |
|    enc.enc.0.conv.norm           |    64                  |    26.214M |
|   enc.enc.1.conv                 |   9.312K               |   0.384G   |
|    enc.enc.1.conv.conv           |    9.248K              |    0.377G  |
|    enc.enc.1.conv.norm           |    64                  |    6.554M  |
|  dec                             |  46.401K               |  3.078G    |
|   dec.dec                        |   46.368K              |   3.072G   |
|    dec.dec.0.conv                |    37.056K             |    1.536G  |
|    dec.dec.1.conv                |    9.312K              |    1.536G  |
|   dec.readout                    |   33                   |   5.243M   |
|    dec.readout.weight            |    (1, 32, 1, 1)       |            |
|    dec.readout.bias              |    (1,)                |            |
|  hid.enc                         |  11.849M               |  45.83G    |
|   hid.enc.0                      |   2.165M               |   8.53G    |
|    hid.enc.0.block               |    2.083M              |    8.195G  |
|    hid.enc.0.reduction           |    82.176K             |    0.336G  |
|   hid.enc.1.block                |   1.372M               |   5.281G   |
|    hid.enc.1.block.layer_scale_1 |    (256,)              |            |
|    hid.enc.1.block.layer_scale_2 |    (256,)              |            |
|    hid.enc.1.block.norm1         |    0.512K              |    2.097M  |
|    hid.enc.1.block.attn          |    0.299M              |    0.906G  |
|    hid.enc.1.block.norm2         |    0.512K              |    2.097M  |
|    hid.enc.1.block.mlp           |    1.071M              |    4.37G   |
|   hid.enc.2.block                |   1.372M               |   5.281G   |
|    hid.enc.2.block.layer_scale_1 |    (256,)              |            |
|    hid.enc.2.block.layer_scale_2 |    (256,)              |            |
|    hid.enc.2.block.norm1         |    0.512K              |    2.097M  |
|    hid.enc.2.block.attn          |    0.299M              |    0.906G  |
|    hid.enc.2.block.norm2         |    0.512K              |    2.097M  |
|    hid.enc.2.block.mlp           |    1.071M              |    4.37G   |
|   hid.enc.3.block                |   1.372M               |   5.281G   |
|    hid.enc.3.block.layer_scale_1 |    (256,)              |            |
|    hid.enc.3.block.layer_scale_2 |    (256,)              |            |
|    hid.enc.3.block.norm1         |    0.512K              |    2.097M  |
|    hid.enc.3.block.attn          |    0.299M              |    0.906G  |
|    hid.enc.3.block.norm2         |    0.512K              |    2.097M  |
|    hid.enc.3.block.mlp           |    1.071M              |    4.37G   |
|   hid.enc.4.block                |   1.372M               |   5.281G   |
|    hid.enc.4.block.layer_scale_1 |    (256,)              |            |
|    hid.enc.4.block.layer_scale_2 |    (256,)              |            |
|    hid.enc.4.block.norm1         |    0.512K              |    2.097M  |
|    hid.enc.4.block.attn          |    0.299M              |    0.906G  |
|    hid.enc.4.block.norm2         |    0.512K              |    2.097M  |
|    hid.enc.4.block.mlp           |    1.071M              |    4.37G   |
|   hid.enc.5.block                |   1.372M               |   5.281G   |
|    hid.enc.5.block.layer_scale_1 |    (256,)              |            |
|    hid.enc.5.block.layer_scale_2 |    (256,)              |            |
|    hid.enc.5.block.norm1         |    0.512K              |    2.097M  |
|    hid.enc.5.block.attn          |    0.299M              |    0.906G  |
|    hid.enc.5.block.norm2         |    0.512K              |    2.097M  |
|    hid.enc.5.block.mlp           |    1.071M              |    4.37G   |
|   hid.enc.6.block                |   1.372M               |   5.281G   |
|    hid.enc.6.block.layer_scale_1 |    (256,)              |            |
|    hid.enc.6.block.layer_scale_2 |    (256,)              |            |
|    hid.enc.6.block.norm1         |    0.512K              |    2.097M  |
|    hid.enc.6.block.attn          |    0.299M              |    0.906G  |
|    hid.enc.6.block.norm2         |    0.512K              |    2.097M  |
|    hid.enc.6.block.mlp           |    1.071M              |    4.37G   |
|   hid.enc.7                      |   1.454M               |   5.616G   |
|    hid.enc.7.block               |    1.372M              |    5.281G  |
|    hid.enc.7.reduction           |    82.24K              |    0.336G  |
Throughputs of sau: 157.551
--------------------------------------------------------------------------------

2024-03-15 11:14:09,309 - Epoch: 1, Steps: 235 | Lr: 4.00945e-06 | Train Loss: 1.27929e+00
2024-03-15 11:14:09,309 - Epoch time: 26.41s, Average time: 26.41s
2024-03-15 11:14:09,310 - Loss decreased (inf --> 1.27929e+00).  Saving best model ...
2024-03-15 11:14:09,769 - 
2024-03-15 11:14:36,009 - Epoch: 2, Steps: 235 | Lr: 4.03781e-06 | Train Loss: 4.75290e-01
2024-03-15 11:14:36,009 - Epoch time: 26.24s, Average time: 26.32s
2024-03-15 11:14:36,012 - Loss decreased (1.27929e+00 --> 4.75290e-01).  Saving best model ...
2024-03-15 11:14:36,428 - 
2024-03-15 11:15:02,445 - Epoch: 3, Steps: 235 | Lr: 4.08505e-06 | Train Loss: 2.70229e-01
2024-03-15 11:15:02,445 - Epoch time: 26.02s, Average time: 26.22s
2024-03-15 11:15:02,445 - Loss decreased (4.75290e-01 --> 2.70229e-01).  Saving best model ...
2024-03-15 11:15:02,888 - 
2024-03-15 11:15:28,808 - Epoch: 4, Steps: 235 | Lr: 4.15117e-06 | Train Loss: 1.61720e-01
2024-03-15 11:15:28,809 - Epoch time: 25.92s, Average time: 26.15s
2024-03-15 11:15:28,809 - Loss decreased (2.70229e-01 --> 1.61720e-01).  Saving best model ...
2024-03-15 11:15:29,240 - 
2024-03-15 11:15:55,719 - Epoch: 5, Steps: 235 | Lr: 4.23614e-06 | Train Loss: 1.17981e-01
2024-03-15 11:15:55,720 - Epoch time: 26.48s, Average time: 26.21s
2024-03-15 11:15:55,720 - Loss decreased (1.61720e-01 --> 1.17981e-01).  Saving best model ...
2024-03-15 11:15:56,140 - 
2024-03-15 11:16:21,988 - Epoch: 6, Steps: 235 | Lr: 4.33992e-06 | Train Loss: 9.68973e-02
2024-03-15 11:16:21,988 - Epoch time: 25.85s, Average time: 26.15s
2024-03-15 11:16:21,988 - Loss decreased (1.17981e-01 --> 9.68973e-02).  Saving best model ...
2024-03-15 11:16:22,401 - 
2024-03-15 11:16:48,178 - Epoch: 7, Steps: 235 | Lr: 4.46247e-06 | Train Loss: 8.23994e-02
2024-03-15 11:16:48,179 - Epoch time: 25.78s, Average time: 26.10s
2024-03-15 11:16:48,179 - Loss decreased (9.68973e-02 --> 8.23994e-02).  Saving best model ...
2024-03-15 11:16:48,596 - 
2024-03-15 11:17:14,523 - Epoch: 8, Steps: 235 | Lr: 4.60374e-06 | Train Loss: 7.08542e-02
2024-03-15 11:17:14,524 - Epoch time: 25.93s, Average time: 26.08s
2024-03-15 11:17:14,524 - Loss decreased (8.23994e-02 --> 7.08542e-02).  Saving best model ...
2024-03-15 11:17:14,942 - 
2024-03-15 11:17:40,882 - Epoch: 9, Steps: 235 | Lr: 4.76368e-06 | Train Loss: 6.25326e-02
2024-03-15 11:17:40,882 - Epoch time: 25.94s, Average time: 26.06s
2024-03-15 11:17:40,882 - Loss decreased (7.08542e-02 --> 6.25326e-02).  Saving best model ...
2024-03-15 11:17:41,352 - 
2024-03-15 11:18:07,539 - Epoch: 10, Steps: 235 | Lr: 4.94223e-06 | Train Loss: 5.55116e-02
2024-03-15 11:18:07,540 - Epoch time: 26.19s, Average time: 26.07s
2024-03-15 11:18:07,540 - Loss decreased (6.25326e-02 --> 5.55116e-02).  Saving best model ...
2024-03-15 11:18:07,996 - 
2024-03-15 11:18:33,983 - Epoch: 11, Steps: 235 | Lr: 5.13932e-06 | Train Loss: 4.93345e-02
2024-03-15 11:18:33,983 - Epoch time: 25.99s, Average time: 26.07s
2024-03-15 11:18:33,984 - Loss decreased (5.55116e-02 --> 4.93345e-02).  Saving best model ...
2024-03-15 11:18:34,396 - 
2024-03-15 11:19:00,316 - Epoch: 12, Steps: 235 | Lr: 5.35486e-06 | Train Loss: 4.48784e-02
2024-03-15 11:19:00,317 - Epoch time: 25.92s, Average time: 26.05s
2024-03-15 11:19:00,317 - Loss decreased (4.93345e-02 --> 4.48784e-02).  Saving best model ...
2024-03-15 11:19:00,740 - 
2024-03-15 11:19:26,511 - Epoch: 13, Steps: 235 | Lr: 5.58877e-06 | Train Loss: 4.13015e-02
2024-03-15 11:19:26,511 - Epoch time: 25.77s, Average time: 26.03s
2024-03-15 11:19:26,512 - Loss decreased (4.48784e-02 --> 4.13015e-02).  Saving best model ...
2024-03-15 11:19:26,924 - 
2024-03-15 11:19:52,703 - Epoch: 14, Steps: 235 | Lr: 5.84096e-06 | Train Loss: 3.81501e-02
2024-03-15 11:19:52,703 - Epoch time: 25.78s, Average time: 26.01s
2024-03-15 11:19:52,704 - Loss decreased (4.13015e-02 --> 3.81501e-02).  Saving best model ...
2024-03-15 11:19:53,114 - 
2024-03-15 11:20:19,228 - Epoch: 15, Steps: 235 | Lr: 6.11133e-06 | Train Loss: 3.54714e-02
2024-03-15 11:20:19,228 - Epoch time: 26.11s, Average time: 26.02s
2024-03-15 11:20:19,229 - Loss decreased (3.81501e-02 --> 3.54714e-02).  Saving best model ...
2024-03-15 11:20:19,663 - 
2024-03-15 11:20:45,473 - Epoch: 16, Steps: 235 | Lr: 6.39978e-06 | Train Loss: 3.30356e-02
2024-03-15 11:20:45,473 - Epoch time: 25.81s, Average time: 26.01s
2024-03-15 11:20:45,473 - Loss decreased (3.54714e-02 --> 3.30356e-02).  Saving best model ...
2024-03-15 11:20:45,886 - 
2024-03-15 11:21:12,081 - Epoch: 17, Steps: 235 | Lr: 6.70619e-06 | Train Loss: 3.12068e-02
2024-03-15 11:21:12,081 - Epoch time: 26.19s, Average time: 26.02s
2024-03-15 11:21:12,082 - Loss decreased (3.30356e-02 --> 3.12068e-02).  Saving best model ...
2024-03-15 11:21:12,497 - 
2024-03-15 11:21:38,463 - Epoch: 18, Steps: 235 | Lr: 7.03044e-06 | Train Loss: 2.93186e-02
2024-03-15 11:21:38,463 - Epoch time: 25.97s, Average time: 26.02s
2024-03-15 11:21:38,463 - Loss decreased (3.12068e-02 --> 2.93186e-02).  Saving best model ...
2024-03-15 11:21:38,883 - 
2024-03-15 11:22:04,891 - Epoch: 19, Steps: 235 | Lr: 7.37240e-06 | Train Loss: 2.72562e-02
2024-03-15 11:22:04,891 - Epoch time: 26.01s, Average time: 26.02s
2024-03-15 11:22:04,891 - Loss decreased (2.93186e-02 --> 2.72562e-02).  Saving best model ...
2024-03-15 11:22:05,295 - 
2024-03-15 11:22:31,045 - Epoch: 20, Steps: 235 | Lr: 7.73194e-06 | Train Loss: 2.61846e-02
2024-03-15 11:22:31,045 - Epoch time: 25.75s, Average time: 26.00s
2024-03-15 11:22:31,045 - Loss decreased (2.72562e-02 --> 2.61846e-02).  Saving best model ...
2024-03-15 11:22:31,450 - 
2024-03-15 11:22:57,409 - Epoch: 21, Steps: 235 | Lr: 8.10891e-06 | Train Loss: 2.44465e-02
2024-03-15 11:22:57,409 - Epoch time: 25.96s, Average time: 26.00s
2024-03-15 11:22:57,409 - Loss decreased (2.61846e-02 --> 2.44465e-02).  Saving best model ...
2024-03-15 11:22:57,839 - 
2024-03-15 11:23:23,729 - Epoch: 22, Steps: 235 | Lr: 8.50318e-06 | Train Loss: 2.33961e-02
2024-03-15 11:23:23,729 - Epoch time: 25.89s, Average time: 25.99s
2024-03-15 11:23:23,729 - Loss decreased (2.44465e-02 --> 2.33961e-02).  Saving best model ...
2024-03-15 11:23:24,140 - 
2024-03-15 11:23:49,999 - Epoch: 23, Steps: 235 | Lr: 8.91457e-06 | Train Loss: 2.20971e-02
2024-03-15 11:23:50,000 - Epoch time: 25.86s, Average time: 25.99s
2024-03-15 11:23:50,000 - Loss decreased (2.33961e-02 --> 2.20971e-02).  Saving best model ...
2024-03-15 11:23:50,549 - 
2024-03-15 11:24:16,463 - Epoch: 24, Steps: 235 | Lr: 9.34294e-06 | Train Loss: 2.10903e-02
2024-03-15 11:24:16,463 - Epoch time: 25.91s, Average time: 25.99s
2024-03-15 11:24:16,463 - Loss decreased (2.20971e-02 --> 2.10903e-02).  Saving best model ...
2024-03-15 11:24:16,902 - 
2024-03-15 11:24:42,897 - Epoch: 25, Steps: 235 | Lr: 9.78810e-06 | Train Loss: 2.02170e-02
2024-03-15 11:24:42,897 - Epoch time: 25.99s, Average time: 25.99s
2024-03-15 11:24:42,897 - Loss decreased (2.10903e-02 --> 2.02170e-02).  Saving best model ...
2024-03-15 11:24:43,337 - 
2024-03-15 11:25:09,309 - Epoch: 26, Steps: 235 | Lr: 1.02499e-05 | Train Loss: 1.97314e-02
2024-03-15 11:25:09,310 - Epoch time: 25.97s, Average time: 25.99s
2024-03-15 11:25:09,310 - Loss decreased (2.02170e-02 --> 1.97314e-02).  Saving best model ...
2024-03-15 11:25:09,749 - 
2024-03-15 11:25:35,601 - Epoch: 27, Steps: 235 | Lr: 1.07281e-05 | Train Loss: 1.81512e-02
2024-03-15 11:25:35,602 - Epoch time: 25.85s, Average time: 25.98s
2024-03-15 11:25:35,602 - Loss decreased (1.97314e-02 --> 1.81512e-02).  Saving best model ...
2024-03-15 11:25:36,025 - 
2024-03-15 11:26:01,947 - Epoch: 28, Steps: 235 | Lr: 1.12226e-05 | Train Loss: 1.81912e-02
2024-03-15 11:26:01,947 - Epoch time: 25.92s, Average time: 25.98s
2024-03-15 11:26:02,263 - 
2024-03-15 11:26:28,332 - Epoch: 29, Steps: 235 | Lr: 1.17332e-05 | Train Loss: 1.79764e-02
2024-03-15 11:26:28,332 - Epoch time: 26.07s, Average time: 25.98s
2024-03-15 11:26:28,332 - Loss decreased (1.81512e-02 --> 1.79764e-02).  Saving best model ...
2024-03-15 11:26:28,756 - 
2024-03-15 11:26:55,204 - Epoch: 30, Steps: 235 | Lr: 1.22596e-05 | Train Loss: 1.65892e-02
2024-03-15 11:26:55,204 - Epoch time: 26.45s, Average time: 26.00s
2024-03-15 11:26:55,204 - Loss decreased (1.79764e-02 --> 1.65892e-02).  Saving best model ...
2024-03-15 11:26:55,623 - 
2024-03-15 11:27:21,576 - Epoch: 31, Steps: 235 | Lr: 1.28017e-05 | Train Loss: 1.56768e-02
2024-03-15 11:27:21,576 - Epoch time: 25.95s, Average time: 26.00s
2024-03-15 11:27:21,576 - Loss decreased (1.65892e-02 --> 1.56768e-02).  Saving best model ...
2024-03-15 11:27:22,013 - 
2024-03-15 11:27:47,888 - Epoch: 32, Steps: 235 | Lr: 1.33592e-05 | Train Loss: 1.49793e-02
2024-03-15 11:27:47,889 - Epoch time: 25.87s, Average time: 25.99s
2024-03-15 11:27:47,889 - Loss decreased (1.56768e-02 --> 1.49793e-02).  Saving best model ...
2024-03-15 11:27:48,323 - 
2024-03-15 11:28:14,351 - Epoch: 33, Steps: 235 | Lr: 1.39319e-05 | Train Loss: 1.47179e-02
2024-03-15 11:28:14,352 - Epoch time: 26.03s, Average time: 25.99s
2024-03-15 11:28:14,352 - Loss decreased (1.49793e-02 --> 1.47179e-02).  Saving best model ...
2024-03-15 11:28:14,880 - 
2024-03-15 11:28:40,827 - Epoch: 34, Steps: 235 | Lr: 1.45196e-05 | Train Loss: 1.40173e-02
2024-03-15 11:28:40,827 - Epoch time: 25.95s, Average time: 25.99s
2024-03-15 11:28:40,827 - Loss decreased (1.47179e-02 --> 1.40173e-02).  Saving best model ...
2024-03-15 11:28:41,243 - 
2024-03-15 11:29:07,185 - Epoch: 35, Steps: 235 | Lr: 1.51221e-05 | Train Loss: 1.36626e-02
2024-03-15 11:29:07,186 - Epoch time: 25.94s, Average time: 25.99s
2024-03-15 11:29:07,186 - Loss decreased (1.40173e-02 --> 1.36626e-02).  Saving best model ...
2024-03-15 11:29:07,600 - 
2024-03-15 11:29:33,439 - Epoch: 36, Steps: 235 | Lr: 1.57391e-05 | Train Loss: 1.39355e-02
2024-03-15 11:29:33,439 - Epoch time: 25.84s, Average time: 25.99s
2024-03-15 11:29:33,749 - 
2024-03-15 11:29:59,689 - Epoch: 37, Steps: 235 | Lr: 1.63704e-05 | Train Loss: 1.23327e-02
2024-03-15 11:29:59,690 - Epoch time: 25.94s, Average time: 25.98s
2024-03-15 11:29:59,690 - Loss decreased (1.36626e-02 --> 1.23327e-02).  Saving best model ...
2024-03-15 11:30:00,111 - 
2024-03-15 11:30:25,883 - Epoch: 38, Steps: 235 | Lr: 1.70157e-05 | Train Loss: 1.20005e-02
2024-03-15 11:30:25,883 - Epoch time: 25.77s, Average time: 25.98s
2024-03-15 11:30:25,883 - Loss decreased (1.23327e-02 --> 1.20005e-02).  Saving best model ...
2024-03-15 11:30:26,300 - 
2024-03-15 11:30:52,165 - Epoch: 39, Steps: 235 | Lr: 1.76748e-05 | Train Loss: 1.28351e-02
2024-03-15 11:30:52,165 - Epoch time: 25.87s, Average time: 25.98s
2024-03-15 11:30:52,495 - 
2024-03-15 11:31:18,427 - Epoch: 40, Steps: 235 | Lr: 1.83474e-05 | Train Loss: 1.15810e-02
2024-03-15 11:31:18,427 - Epoch time: 25.93s, Average time: 25.98s
2024-03-15 11:31:18,427 - Loss decreased (1.20005e-02 --> 1.15810e-02).  Saving best model ...
2024-03-15 11:31:18,867 - 
2024-03-15 11:31:44,833 - Epoch: 41, Steps: 235 | Lr: 1.90333e-05 | Train Loss: 1.12704e-02
2024-03-15 11:31:44,833 - Epoch time: 25.97s, Average time: 25.97s
2024-03-15 11:31:44,834 - Loss decreased (1.15810e-02 --> 1.12704e-02).  Saving best model ...
2024-03-15 11:31:45,257 - 
2024-03-15 11:32:11,062 - Epoch: 42, Steps: 235 | Lr: 1.97322e-05 | Train Loss: 1.08946e-02
2024-03-15 11:32:11,062 - Epoch time: 25.80s, Average time: 25.97s
2024-03-15 11:32:11,062 - Loss decreased (1.12704e-02 --> 1.08946e-02).  Saving best model ...
2024-03-15 11:32:11,478 - 
2024-03-15 11:32:37,273 - Epoch: 43, Steps: 235 | Lr: 2.04438e-05 | Train Loss: 1.08956e-02
2024-03-15 11:32:37,273 - Epoch time: 25.79s, Average time: 25.97s
2024-03-15 11:32:37,584 - 
2024-03-15 11:33:03,540 - Epoch: 44, Steps: 235 | Lr: 2.11678e-05 | Train Loss: 1.02496e-02
2024-03-15 11:33:03,540 - Epoch time: 25.96s, Average time: 25.97s
2024-03-15 11:33:03,540 - Loss decreased (1.08946e-02 --> 1.02496e-02).  Saving best model ...
2024-03-15 11:33:03,988 - 
2024-03-15 11:33:30,028 - Epoch: 45, Steps: 235 | Lr: 2.19039e-05 | Train Loss: 1.05275e-02
2024-03-15 11:33:30,028 - Epoch time: 26.04s, Average time: 25.97s
2024-03-15 11:33:30,358 - 
2024-03-15 11:33:56,221 - Epoch: 46, Steps: 235 | Lr: 2.26519e-05 | Train Loss: 1.02346e-02
2024-03-15 11:33:56,221 - Epoch time: 25.86s, Average time: 25.97s
2024-03-15 11:33:56,222 - Loss decreased (1.02496e-02 --> 1.02346e-02).  Saving best model ...
2024-03-15 11:33:56,668 - 
2024-03-15 11:34:23,042 - Epoch: 47, Steps: 235 | Lr: 2.34115e-05 | Train Loss: 9.44860e-03
2024-03-15 11:34:23,042 - Epoch time: 26.37s, Average time: 25.97s
2024-03-15 11:34:23,042 - Loss decreased (1.02346e-02 --> 9.44860e-03).  Saving best model ...
2024-03-15 11:34:23,485 - 
2024-03-15 11:34:49,468 - Epoch: 48, Steps: 235 | Lr: 2.41823e-05 | Train Loss: 9.58144e-03
2024-03-15 11:34:49,468 - Epoch time: 25.98s, Average time: 25.97s
2024-03-15 11:34:49,779 - 
2024-03-15 11:35:15,765 - Epoch: 49, Steps: 235 | Lr: 2.49641e-05 | Train Loss: 9.09400e-03
2024-03-15 11:35:15,765 - Epoch time: 25.99s, Average time: 25.97s
2024-03-15 11:35:15,765 - Loss decreased (9.44860e-03 --> 9.09400e-03).  Saving best model ...
2024-03-15 11:35:16,179 - 
2024-03-15 11:35:42,133 - Epoch: 50, Steps: 235 | Lr: 2.57565e-05 | Train Loss: 8.80827e-03
2024-03-15 11:35:42,133 - Epoch time: 25.95s, Average time: 25.97s
2024-03-15 11:35:42,134 - Loss decreased (9.09400e-03 --> 8.80827e-03).  Saving best model ...
2024-03-15 11:35:42,243 - Save the loss_recoder of 50 epochs
2024-03-15 11:35:42,332 - Simulation picture saved in /home/bingxing2/home/scx6mfb/mhy/SAU/CFD-CNN/Model/SAU_1001_10to1-0.3/pic/loss_record.png
2024-03-15 11:35:43,512 - 
2024-03-15 11:36:09,240 - Epoch: 51, Steps: 235 | Lr: 2.65593e-05 | Train Loss: 8.41917e-03
2024-03-15 11:36:09,240 - Epoch time: 25.73s, Average time: 25.97s
2024-03-15 11:36:09,240 - Loss decreased (8.80827e-03 --> 8.41917e-03).  Saving best model ...
2024-03-15 11:36:09,672 - 
2024-03-15 11:36:35,529 - Epoch: 52, Steps: 235 | Lr: 2.73720e-05 | Train Loss: 8.09304e-03
2024-03-15 11:36:35,530 - Epoch time: 25.86s, Average time: 25.97s
2024-03-15 11:36:35,530 - Loss decreased (8.41917e-03 --> 8.09304e-03).  Saving best model ...
2024-03-15 11:36:35,946 - 
2024-03-15 11:37:02,214 - Epoch: 53, Steps: 235 | Lr: 2.81945e-05 | Train Loss: 7.85514e-03
2024-03-15 11:37:02,214 - Epoch time: 26.27s, Average time: 25.97s
2024-03-15 11:37:02,214 - Loss decreased (8.09304e-03 --> 7.85514e-03).  Saving best model ...
2024-03-15 11:37:02,639 - 
2024-03-15 11:37:28,562 - Epoch: 54, Steps: 235 | Lr: 2.90264e-05 | Train Loss: 7.89948e-03
2024-03-15 11:37:28,562 - Epoch time: 25.92s, Average time: 25.97s
2024-03-15 11:37:28,883 - 
2024-03-15 11:37:54,688 - Epoch: 55, Steps: 235 | Lr: 2.98673e-05 | Train Loss: 7.95434e-03
2024-03-15 11:37:54,688 - Epoch time: 25.80s, Average time: 25.97s
2024-03-15 11:37:55,002 - 
2024-03-15 11:38:20,650 - Epoch: 56, Steps: 235 | Lr: 3.07169e-05 | Train Loss: 7.69693e-03
2024-03-15 11:38:20,650 - Epoch time: 25.65s, Average time: 25.96s
2024-03-15 11:38:20,651 - Loss decreased (7.85514e-03 --> 7.69693e-03).  Saving best model ...
2024-03-15 11:38:21,070 - 
2024-03-15 11:38:46,944 - Epoch: 57, Steps: 235 | Lr: 3.15749e-05 | Train Loss: 7.85320e-03
2024-03-15 11:38:46,944 - Epoch time: 25.87s, Average time: 25.96s
2024-03-15 11:38:47,269 - 
2024-03-15 11:39:13,115 - Epoch: 58, Steps: 235 | Lr: 3.24410e-05 | Train Loss: 7.38446e-03
2024-03-15 11:39:13,116 - Epoch time: 25.85s, Average time: 25.96s
2024-03-15 11:39:13,116 - Loss decreased (7.69693e-03 --> 7.38446e-03).  Saving best model ...
2024-03-15 11:39:13,557 - 
2024-03-15 11:39:39,466 - Epoch: 59, Steps: 235 | Lr: 3.33147e-05 | Train Loss: 7.23496e-03
2024-03-15 11:39:39,467 - Epoch time: 25.91s, Average time: 25.96s
2024-03-15 11:39:39,467 - Loss decreased (7.38446e-03 --> 7.23496e-03).  Saving best model ...
2024-03-15 11:39:39,878 - 
2024-03-15 11:40:05,669 - Epoch: 60, Steps: 235 | Lr: 3.41958e-05 | Train Loss: 6.91808e-03
2024-03-15 11:40:05,669 - Epoch time: 25.79s, Average time: 25.96s
2024-03-15 11:40:05,669 - Loss decreased (7.23496e-03 --> 6.91808e-03).  Saving best model ...
2024-03-15 11:40:06,114 - 
2024-03-15 11:40:31,886 - Epoch: 61, Steps: 235 | Lr: 3.50840e-05 | Train Loss: 6.76229e-03
2024-03-15 11:40:31,886 - Epoch time: 25.77s, Average time: 25.95s
2024-03-15 11:40:31,886 - Loss decreased (6.91808e-03 --> 6.76229e-03).  Saving best model ...
2024-03-15 11:40:32,324 - 
2024-03-15 11:40:58,313 - Epoch: 62, Steps: 235 | Lr: 3.59788e-05 | Train Loss: 6.75724e-03
2024-03-15 11:40:58,313 - Epoch time: 25.99s, Average time: 25.95s
2024-03-15 11:40:58,313 - Loss decreased (6.76229e-03 --> 6.75724e-03).  Saving best model ...
2024-03-15 11:40:58,738 - 
2024-03-15 11:41:24,331 - Epoch: 63, Steps: 235 | Lr: 3.68799e-05 | Train Loss: 6.51506e-03
2024-03-15 11:41:24,331 - Epoch time: 25.59s, Average time: 25.95s
2024-03-15 11:41:24,332 - Loss decreased (6.75724e-03 --> 6.51506e-03).  Saving best model ...
2024-03-15 11:41:24,748 - 
2024-03-15 11:41:50,380 - Epoch: 64, Steps: 235 | Lr: 3.77869e-05 | Train Loss: 6.22211e-03
2024-03-15 11:41:50,380 - Epoch time: 25.63s, Average time: 25.94s
2024-03-15 11:41:50,381 - Loss decreased (6.51506e-03 --> 6.22211e-03).  Saving best model ...
2024-03-15 11:41:50,797 - 
2024-03-15 11:42:16,670 - Epoch: 65, Steps: 235 | Lr: 3.86996e-05 | Train Loss: 6.18068e-03
2024-03-15 11:42:16,670 - Epoch time: 25.87s, Average time: 25.94s
2024-03-15 11:42:16,670 - Loss decreased (6.22211e-03 --> 6.18068e-03).  Saving best model ...
2024-03-15 11:42:17,099 - 
2024-03-15 11:42:43,253 - Epoch: 66, Steps: 235 | Lr: 3.96175e-05 | Train Loss: 6.21356e-03
2024-03-15 11:42:43,254 - Epoch time: 26.15s, Average time: 25.95s
2024-03-15 11:42:43,575 - 
2024-03-15 11:43:09,721 - Epoch: 67, Steps: 235 | Lr: 4.05403e-05 | Train Loss: 5.93492e-03
2024-03-15 11:43:09,721 - Epoch time: 26.15s, Average time: 25.95s
2024-03-15 11:43:09,722 - Loss decreased (6.18068e-03 --> 5.93492e-03).  Saving best model ...
2024-03-15 11:43:10,156 - 
2024-03-15 11:43:35,852 - Epoch: 68, Steps: 235 | Lr: 4.14675e-05 | Train Loss: 5.73144e-03
2024-03-15 11:43:35,852 - Epoch time: 25.70s, Average time: 25.94s
2024-03-15 11:43:35,853 - Loss decreased (5.93492e-03 --> 5.73144e-03).  Saving best model ...
2024-03-15 11:43:36,272 - 
2024-03-15 11:44:02,115 - Epoch: 69, Steps: 235 | Lr: 4.23990e-05 | Train Loss: 5.87719e-03
2024-03-15 11:44:02,116 - Epoch time: 25.84s, Average time: 25.94s
2024-03-15 11:44:02,435 - 
2024-03-15 11:44:28,274 - Epoch: 70, Steps: 235 | Lr: 4.33342e-05 | Train Loss: 5.81210e-03
2024-03-15 11:44:28,274 - Epoch time: 25.84s, Average time: 25.94s
2024-03-15 11:44:28,594 - 
2024-03-15 11:44:54,311 - Epoch: 71, Steps: 235 | Lr: 4.42728e-05 | Train Loss: 5.76179e-03
2024-03-15 11:44:54,311 - Epoch time: 25.72s, Average time: 25.94s
2024-03-15 11:44:54,631 - 
2024-03-15 11:45:20,730 - Epoch: 72, Steps: 235 | Lr: 4.52145e-05 | Train Loss: 5.39849e-03
2024-03-15 11:45:20,730 - Epoch time: 26.10s, Average time: 25.94s
2024-03-15 11:45:20,730 - Loss decreased (5.73144e-03 --> 5.39849e-03).  Saving best model ...
2024-03-15 11:45:21,151 - 
2024-03-15 11:45:46,890 - Epoch: 73, Steps: 235 | Lr: 4.61588e-05 | Train Loss: 5.72405e-03
2024-03-15 11:45:46,890 - Epoch time: 25.74s, Average time: 25.94s
2024-03-15 11:45:47,217 - 
2024-03-15 11:46:12,930 - Epoch: 74, Steps: 235 | Lr: 4.71055e-05 | Train Loss: 5.07136e-03
2024-03-15 11:46:12,930 - Epoch time: 25.71s, Average time: 25.93s
2024-03-15 11:46:12,930 - Loss decreased (5.39849e-03 --> 5.07136e-03).  Saving best model ...
2024-03-15 11:46:13,343 - 
2024-03-15 11:46:38,768 - Epoch: 75, Steps: 235 | Lr: 4.80541e-05 | Train Loss: 4.94217e-03
2024-03-15 11:46:38,768 - Epoch time: 25.42s, Average time: 25.93s
2024-03-15 11:46:38,769 - Loss decreased (5.07136e-03 --> 4.94217e-03).  Saving best model ...
2024-03-15 11:46:39,217 - 
2024-03-15 11:47:04,868 - Epoch: 76, Steps: 235 | Lr: 4.90042e-05 | Train Loss: 4.94840e-03
2024-03-15 11:47:04,868 - Epoch time: 25.65s, Average time: 25.92s
2024-03-15 11:47:05,182 - 
2024-03-15 11:47:31,159 - Epoch: 77, Steps: 235 | Lr: 4.99555e-05 | Train Loss: 5.02138e-03
2024-03-15 11:47:31,160 - Epoch time: 25.98s, Average time: 25.92s
2024-03-15 11:47:31,471 - 
2024-03-15 11:47:57,204 - Epoch: 78, Steps: 235 | Lr: 5.09076e-05 | Train Loss: 4.73049e-03
2024-03-15 11:47:57,204 - Epoch time: 25.73s, Average time: 25.92s
2024-03-15 11:47:57,205 - Loss decreased (4.94217e-03 --> 4.73049e-03).  Saving best model ...
2024-03-15 11:47:57,641 - 
2024-03-15 11:48:23,393 - Epoch: 79, Steps: 235 | Lr: 5.18601e-05 | Train Loss: 4.87942e-03
2024-03-15 11:48:23,394 - Epoch time: 25.75s, Average time: 25.92s
2024-03-15 11:48:23,721 - 
2024-03-15 11:48:49,511 - Epoch: 80, Steps: 235 | Lr: 5.28127e-05 | Train Loss: 4.72486e-03
2024-03-15 11:48:49,511 - Epoch time: 25.79s, Average time: 25.92s
2024-03-15 11:48:49,511 - Loss decreased (4.73049e-03 --> 4.72486e-03).  Saving best model ...
2024-03-15 11:48:49,919 - 
2024-03-15 11:49:15,640 - Epoch: 81, Steps: 235 | Lr: 5.37650e-05 | Train Loss: 4.72966e-03
2024-03-15 11:49:15,641 - Epoch time: 25.72s, Average time: 25.92s
2024-03-15 11:49:15,972 - 
2024-03-15 11:49:41,806 - Epoch: 82, Steps: 235 | Lr: 5.47166e-05 | Train Loss: 4.38207e-03
2024-03-15 11:49:41,806 - Epoch time: 25.83s, Average time: 25.92s
2024-03-15 11:49:41,806 - Loss decreased (4.72486e-03 --> 4.38207e-03).  Saving best model ...
2024-03-15 11:49:42,231 - 
2024-03-15 11:50:07,896 - Epoch: 83, Steps: 235 | Lr: 5.56671e-05 | Train Loss: 4.15639e-03
2024-03-15 11:50:07,896 - Epoch time: 25.67s, Average time: 25.91s
2024-03-15 11:50:07,897 - Loss decreased (4.38207e-03 --> 4.15639e-03).  Saving best model ...
2024-03-15 11:50:08,443 - 
2024-03-15 11:50:34,236 - Epoch: 84, Steps: 235 | Lr: 5.66162e-05 | Train Loss: 4.15954e-03
2024-03-15 11:50:34,236 - Epoch time: 25.79s, Average time: 25.91s
2024-03-15 11:50:34,557 - 
2024-03-15 11:51:00,162 - Epoch: 85, Steps: 235 | Lr: 5.75634e-05 | Train Loss: 4.44663e-03
2024-03-15 11:51:00,162 - Epoch time: 25.60s, Average time: 25.91s
2024-03-15 11:51:00,483 - 
2024-03-15 11:51:25,984 - Epoch: 86, Steps: 235 | Lr: 5.85085e-05 | Train Loss: 4.04402e-03
2024-03-15 11:51:25,985 - Epoch time: 25.50s, Average time: 25.90s
2024-03-15 11:51:25,985 - Loss decreased (4.15639e-03 --> 4.04402e-03).  Saving best model ...
2024-03-15 11:51:26,420 - 
2024-03-15 11:51:52,009 - Epoch: 87, Steps: 235 | Lr: 5.94510e-05 | Train Loss: 4.44038e-03
2024-03-15 11:51:52,009 - Epoch time: 25.59s, Average time: 25.90s
2024-03-15 11:51:52,324 - 
2024-03-15 11:52:18,030 - Epoch: 88, Steps: 235 | Lr: 6.03906e-05 | Train Loss: 4.05095e-03
2024-03-15 11:52:18,030 - Epoch time: 25.71s, Average time: 25.90s
2024-03-15 11:52:18,340 - 
2024-03-15 11:52:43,855 - Epoch: 89, Steps: 235 | Lr: 6.13268e-05 | Train Loss: 3.64255e-03
2024-03-15 11:52:43,855 - Epoch time: 25.51s, Average time: 25.89s
2024-03-15 11:52:43,855 - Loss decreased (4.04402e-03 --> 3.64255e-03).  Saving best model ...
2024-03-15 11:52:44,296 - 
2024-03-15 11:53:10,161 - Epoch: 90, Steps: 235 | Lr: 6.22594e-05 | Train Loss: 3.75931e-03
2024-03-15 11:53:10,161 - Epoch time: 25.86s, Average time: 25.89s
2024-03-15 11:53:10,478 - 
2024-03-15 11:53:36,104 - Epoch: 91, Steps: 235 | Lr: 6.31879e-05 | Train Loss: 3.72848e-03
2024-03-15 11:53:36,104 - Epoch time: 25.63s, Average time: 25.89s
2024-03-15 11:53:36,417 - 
2024-03-15 11:54:02,005 - Epoch: 92, Steps: 235 | Lr: 6.41121e-05 | Train Loss: 3.77021e-03
2024-03-15 11:54:02,005 - Epoch time: 25.59s, Average time: 25.89s
2024-03-15 11:54:02,344 - 
2024-03-15 11:54:28,025 - Epoch: 93, Steps: 235 | Lr: 6.50314e-05 | Train Loss: 3.46519e-03
2024-03-15 11:54:28,026 - Epoch time: 25.68s, Average time: 25.88s
2024-03-15 11:54:28,026 - Loss decreased (3.64255e-03 --> 3.46519e-03).  Saving best model ...
2024-03-15 11:54:28,435 - 
2024-03-15 11:54:54,375 - Epoch: 94, Steps: 235 | Lr: 6.59457e-05 | Train Loss: 3.40356e-03
2024-03-15 11:54:54,376 - Epoch time: 25.94s, Average time: 25.88s
2024-03-15 11:54:54,376 - Loss decreased (3.46519e-03 --> 3.40356e-03).  Saving best model ...
2024-03-15 11:54:54,819 - 
2024-03-15 11:55:20,404 - Epoch: 95, Steps: 235 | Lr: 6.68544e-05 | Train Loss: 3.43046e-03
2024-03-15 11:55:20,405 - Epoch time: 25.59s, Average time: 25.88s
2024-03-15 11:55:20,729 - 
2024-03-15 11:55:46,569 - Epoch: 96, Steps: 235 | Lr: 6.77573e-05 | Train Loss: 3.29107e-03
2024-03-15 11:55:46,569 - Epoch time: 25.84s, Average time: 25.88s
2024-03-15 11:55:46,570 - Loss decreased (3.40356e-03 --> 3.29107e-03).  Saving best model ...
2024-03-15 11:55:46,988 - 
2024-03-15 11:56:12,738 - Epoch: 97, Steps: 235 | Lr: 6.86540e-05 | Train Loss: 3.31905e-03
2024-03-15 11:56:12,738 - Epoch time: 25.75s, Average time: 25.88s
2024-03-15 11:56:13,054 - 
2024-03-15 11:56:38,644 - Epoch: 98, Steps: 235 | Lr: 6.95441e-05 | Train Loss: 3.57606e-03
2024-03-15 11:56:38,644 - Epoch time: 25.59s, Average time: 25.88s
2024-03-15 11:56:38,959 - 
2024-03-15 11:57:04,237 - Epoch: 99, Steps: 235 | Lr: 7.04273e-05 | Train Loss: 3.24620e-03
2024-03-15 11:57:04,237 - Epoch time: 25.28s, Average time: 25.87s
2024-03-15 11:57:04,238 - Loss decreased (3.29107e-03 --> 3.24620e-03).  Saving best model ...
2024-03-15 11:57:04,664 - 
2024-03-15 11:57:30,092 - Epoch: 100, Steps: 235 | Lr: 7.13033e-05 | Train Loss: 3.02670e-03
2024-03-15 11:57:30,093 - Epoch time: 25.43s, Average time: 25.87s
2024-03-15 11:57:30,093 - Loss decreased (3.24620e-03 --> 3.02670e-03).  Saving best model ...
2024-03-15 11:57:30,201 - Save the loss_recoder of 100 epochs
2024-03-15 11:57:30,222 - Simulation picture saved in /home/bingxing2/home/scx6mfb/mhy/SAU/CFD-CNN/Model/SAU_1001_10to1-0.3/pic/loss_record.png
2024-03-15 11:57:31,158 - 
2024-03-15 11:57:56,634 - Epoch: 101, Steps: 235 | Lr: 7.21716e-05 | Train Loss: 3.00590e-03
2024-03-15 11:57:56,635 - Epoch time: 25.48s, Average time: 25.86s
2024-03-15 11:57:56,635 - Loss decreased (3.02670e-03 --> 3.00590e-03).  Saving best model ...
2024-03-15 11:57:57,051 - 
2024-03-15 11:58:22,642 - Epoch: 102, Steps: 235 | Lr: 7.30320e-05 | Train Loss: 3.16945e-03
2024-03-15 11:58:22,643 - Epoch time: 25.59s, Average time: 25.86s
2024-03-15 11:58:22,957 - 
2024-03-15 11:58:48,586 - Epoch: 103, Steps: 235 | Lr: 7.38841e-05 | Train Loss: 3.14118e-03
2024-03-15 11:58:48,586 - Epoch time: 25.63s, Average time: 25.86s
2024-03-15 11:58:48,899 - 
2024-03-15 11:59:14,321 - Epoch: 104, Steps: 235 | Lr: 7.47276e-05 | Train Loss: 2.92736e-03
2024-03-15 11:59:14,321 - Epoch time: 25.42s, Average time: 25.85s
2024-03-15 11:59:14,321 - Loss decreased (3.00590e-03 --> 2.92736e-03).  Saving best model ...
2024-03-15 11:59:14,867 - 
2024-03-15 11:59:40,468 - Epoch: 105, Steps: 235 | Lr: 7.55622e-05 | Train Loss: 2.84035e-03
2024-03-15 11:59:40,468 - Epoch time: 25.60s, Average time: 25.85s
2024-03-15 11:59:40,468 - Loss decreased (2.92736e-03 --> 2.84035e-03).  Saving best model ...
2024-03-15 11:59:40,896 - 
2024-03-15 12:00:06,864 - Epoch: 106, Steps: 235 | Lr: 7.63875e-05 | Train Loss: 2.93307e-03
2024-03-15 12:00:06,864 - Epoch time: 25.97s, Average time: 25.85s
2024-03-15 12:00:07,196 - 
2024-03-15 12:00:32,720 - Epoch: 107, Steps: 235 | Lr: 7.72031e-05 | Train Loss: 2.96401e-03
2024-03-15 12:00:32,721 - Epoch time: 25.52s, Average time: 25.85s
2024-03-15 12:00:33,045 - 
2024-03-15 12:00:58,459 - Epoch: 108, Steps: 235 | Lr: 7.80089e-05 | Train Loss: 2.77650e-03
2024-03-15 12:00:58,459 - Epoch time: 25.41s, Average time: 25.84s
2024-03-15 12:00:58,459 - Loss decreased (2.84035e-03 --> 2.77650e-03).  Saving best model ...
2024-03-15 12:00:58,879 - 
2024-03-15 12:01:24,316 - Epoch: 109, Steps: 235 | Lr: 7.88044e-05 | Train Loss: 2.79239e-03
2024-03-15 12:01:24,316 - Epoch time: 25.44s, Average time: 25.84s
2024-03-15 12:01:24,647 - 
2024-03-15 12:01:50,172 - Epoch: 110, Steps: 235 | Lr: 7.95893e-05 | Train Loss: 2.82260e-03
2024-03-15 12:01:50,172 - Epoch time: 25.52s, Average time: 25.84s
2024-03-15 12:01:50,489 - 
2024-03-15 12:02:15,973 - Epoch: 111, Steps: 235 | Lr: 8.03634e-05 | Train Loss: 2.52115e-03
2024-03-15 12:02:15,973 - Epoch time: 25.48s, Average time: 25.83s
2024-03-15 12:02:15,973 - Loss decreased (2.77650e-03 --> 2.52115e-03).  Saving best model ...
2024-03-15 12:02:16,394 - 
2024-03-15 12:02:41,841 - Epoch: 112, Steps: 235 | Lr: 8.11263e-05 | Train Loss: 2.69603e-03
2024-03-15 12:02:41,841 - Epoch time: 25.45s, Average time: 25.83s
2024-03-15 12:02:42,153 - 
2024-03-15 12:03:07,660 - Epoch: 113, Steps: 235 | Lr: 8.18777e-05 | Train Loss: 2.59481e-03
2024-03-15 12:03:07,661 - Epoch time: 25.51s, Average time: 25.83s
2024-03-15 12:03:07,988 - 
2024-03-15 12:03:33,459 - Epoch: 114, Steps: 235 | Lr: 8.26173e-05 | Train Loss: 2.68316e-03
2024-03-15 12:03:33,459 - Epoch time: 25.47s, Average time: 25.83s
2024-03-15 12:03:33,775 - 
2024-03-15 12:03:59,459 - Epoch: 115, Steps: 235 | Lr: 8.33449e-05 | Train Loss: 2.55965e-03
2024-03-15 12:03:59,459 - Epoch time: 25.68s, Average time: 25.82s
2024-03-15 12:03:59,790 - 
2024-03-15 12:04:25,248 - Epoch: 116, Steps: 235 | Lr: 8.40602e-05 | Train Loss: 2.41684e-03
2024-03-15 12:04:25,248 - Epoch time: 25.46s, Average time: 25.82s
2024-03-15 12:04:25,248 - Loss decreased (2.52115e-03 --> 2.41684e-03).  Saving best model ...
2024-03-15 12:04:25,681 - 
2024-03-15 12:04:51,340 - Epoch: 117, Steps: 235 | Lr: 8.47628e-05 | Train Loss: 2.49021e-03
2024-03-15 12:04:51,340 - Epoch time: 25.66s, Average time: 25.82s
2024-03-15 12:04:51,653 - 
2024-03-15 12:05:16,959 - Epoch: 118, Steps: 235 | Lr: 8.54525e-05 | Train Loss: 2.53185e-03
2024-03-15 12:05:16,960 - Epoch time: 25.31s, Average time: 25.82s
2024-03-15 12:05:17,276 - 
2024-03-15 12:05:42,891 - Epoch: 119, Steps: 235 | Lr: 8.61291e-05 | Train Loss: 2.23850e-03
2024-03-15 12:05:42,892 - Epoch time: 25.61s, Average time: 25.81s
2024-03-15 12:05:42,892 - Loss decreased (2.41684e-03 --> 2.23850e-03).  Saving best model ...
2024-03-15 12:05:43,302 - 
2024-03-15 12:06:08,751 - Epoch: 120, Steps: 235 | Lr: 8.67922e-05 | Train Loss: 2.29463e-03
2024-03-15 12:06:08,752 - Epoch time: 25.45s, Average time: 25.81s
2024-03-15 12:06:09,084 - 
2024-03-15 12:06:34,427 - Epoch: 121, Steps: 235 | Lr: 8.74416e-05 | Train Loss: 2.34951e-03
2024-03-15 12:06:34,427 - Epoch time: 25.34s, Average time: 25.81s
2024-03-15 12:06:34,745 - 
2024-03-15 12:07:00,557 - Epoch: 122, Steps: 235 | Lr: 8.80770e-05 | Train Loss: 2.26017e-03
2024-03-15 12:07:00,557 - Epoch time: 25.81s, Average time: 25.81s
2024-03-15 12:07:00,882 - 
2024-03-15 12:07:26,371 - Epoch: 123, Steps: 235 | Lr: 8.86982e-05 | Train Loss: 2.25438e-03
2024-03-15 12:07:26,372 - Epoch time: 25.49s, Average time: 25.80s
2024-03-15 12:07:26,685 - 
2024-03-15 12:07:52,254 - Epoch: 124, Steps: 235 | Lr: 8.93050e-05 | Train Loss: 2.15723e-03
2024-03-15 12:07:52,254 - Epoch time: 25.57s, Average time: 25.80s
2024-03-15 12:07:52,254 - Loss decreased (2.23850e-03 --> 2.15723e-03).  Saving best model ...
2024-03-15 12:07:52,706 - 
2024-03-15 12:08:18,257 - Epoch: 125, Steps: 235 | Lr: 8.98971e-05 | Train Loss: 2.15735e-03
2024-03-15 12:08:18,257 - Epoch time: 25.55s, Average time: 25.80s
2024-03-15 12:08:18,574 - 
2024-03-15 12:08:43,971 - Epoch: 126, Steps: 235 | Lr: 9.04742e-05 | Train Loss: 2.08504e-03
2024-03-15 12:08:43,971 - Epoch time: 25.40s, Average time: 25.80s
2024-03-15 12:08:43,972 - Loss decreased (2.15723e-03 --> 2.08504e-03).  Saving best model ...
2024-03-15 12:08:44,393 - 
2024-03-15 12:09:09,931 - Epoch: 127, Steps: 235 | Lr: 9.10362e-05 | Train Loss: 2.13628e-03
2024-03-15 12:09:09,931 - Epoch time: 25.54s, Average time: 25.79s
2024-03-15 12:09:10,255 - 
2024-03-15 12:09:35,778 - Epoch: 128, Steps: 235 | Lr: 9.15829e-05 | Train Loss: 2.08001e-03
2024-03-15 12:09:35,778 - Epoch time: 25.52s, Average time: 25.79s
2024-03-15 12:09:35,779 - Loss decreased (2.08504e-03 --> 2.08001e-03).  Saving best model ...
2024-03-15 12:09:36,191 - 
2024-03-15 12:10:01,753 - Epoch: 129, Steps: 235 | Lr: 9.21139e-05 | Train Loss: 1.98338e-03
2024-03-15 12:10:01,754 - Epoch time: 25.56s, Average time: 25.79s
2024-03-15 12:10:01,754 - Loss decreased (2.08001e-03 --> 1.98338e-03).  Saving best model ...
2024-03-15 12:10:02,179 - 
2024-03-15 12:10:27,850 - Epoch: 130, Steps: 235 | Lr: 9.26291e-05 | Train Loss: 1.98155e-03
2024-03-15 12:10:27,850 - Epoch time: 25.67s, Average time: 25.79s
2024-03-15 12:10:27,850 - Loss decreased (1.98338e-03 --> 1.98155e-03).  Saving best model ...
2024-03-15 12:10:28,264 - 
2024-03-15 12:10:53,688 - Epoch: 131, Steps: 235 | Lr: 9.31284e-05 | Train Loss: 2.01817e-03
2024-03-15 12:10:53,688 - Epoch time: 25.42s, Average time: 25.79s
2024-03-15 12:10:54,010 - 
2024-03-15 12:11:19,448 - Epoch: 132, Steps: 235 | Lr: 9.36114e-05 | Train Loss: 1.83417e-03
2024-03-15 12:11:19,448 - Epoch time: 25.44s, Average time: 25.78s
2024-03-15 12:11:19,449 - Loss decreased (1.98155e-03 --> 1.83417e-03).  Saving best model ...
2024-03-15 12:11:19,863 - 
2024-03-15 12:11:45,330 - Epoch: 133, Steps: 235 | Lr: 9.40780e-05 | Train Loss: 1.80210e-03
2024-03-15 12:11:45,330 - Epoch time: 25.47s, Average time: 25.78s
2024-03-15 12:11:45,330 - Loss decreased (1.83417e-03 --> 1.80210e-03).  Saving best model ...
2024-03-15 12:11:45,758 - 
2024-03-15 12:12:11,873 - Epoch: 134, Steps: 235 | Lr: 9.45281e-05 | Train Loss: 1.98172e-03
2024-03-15 12:12:11,873 - Epoch time: 26.11s, Average time: 25.78s
2024-03-15 12:12:12,194 - 
2024-03-15 12:12:37,697 - Epoch: 135, Steps: 235 | Lr: 9.49614e-05 | Train Loss: 1.84301e-03
2024-03-15 12:12:37,697 - Epoch time: 25.50s, Average time: 25.78s
2024-03-15 12:12:38,022 - 
2024-03-15 12:13:03,827 - Epoch: 136, Steps: 235 | Lr: 9.53778e-05 | Train Loss: 1.80538e-03
2024-03-15 12:13:03,827 - Epoch time: 25.80s, Average time: 25.78s
2024-03-15 12:13:04,160 - 
2024-03-15 12:13:29,533 - Epoch: 137, Steps: 235 | Lr: 9.57771e-05 | Train Loss: 1.74871e-03
2024-03-15 12:13:29,534 - Epoch time: 25.37s, Average time: 25.78s
2024-03-15 12:13:29,534 - Loss decreased (1.80210e-03 --> 1.74871e-03).  Saving best model ...
2024-03-15 12:13:29,964 - 
2024-03-15 12:13:55,384 - Epoch: 138, Steps: 235 | Lr: 9.61592e-05 | Train Loss: 1.68356e-03
2024-03-15 12:13:55,385 - Epoch time: 25.42s, Average time: 25.78s
2024-03-15 12:13:55,385 - Loss decreased (1.74871e-03 --> 1.68356e-03).  Saving best model ...
2024-03-15 12:13:55,797 - 
2024-03-15 12:14:21,199 - Epoch: 139, Steps: 235 | Lr: 9.65238e-05 | Train Loss: 2.41663e-03
2024-03-15 12:14:21,200 - Epoch time: 25.40s, Average time: 25.77s
2024-03-15 12:14:21,527 - 
2024-03-15 12:14:46,904 - Epoch: 140, Steps: 235 | Lr: 9.68710e-05 | Train Loss: 1.65349e-03
2024-03-15 12:14:46,904 - Epoch time: 25.38s, Average time: 25.77s
2024-03-15 12:14:46,904 - Loss decreased (1.68356e-03 --> 1.65349e-03).  Saving best model ...
2024-03-15 12:14:47,330 - 
2024-03-15 12:15:12,800 - Epoch: 141, Steps: 235 | Lr: 9.72004e-05 | Train Loss: 1.58656e-03
2024-03-15 12:15:12,800 - Epoch time: 25.47s, Average time: 25.77s
2024-03-15 12:15:12,800 - Loss decreased (1.65349e-03 --> 1.58656e-03).  Saving best model ...
2024-03-15 12:15:13,220 - 
2024-03-15 12:15:38,734 - Epoch: 142, Steps: 235 | Lr: 9.75121e-05 | Train Loss: 1.56793e-03
2024-03-15 12:15:38,734 - Epoch time: 25.51s, Average time: 25.77s
2024-03-15 12:15:38,735 - Loss decreased (1.58656e-03 --> 1.56793e-03).  Saving best model ...
2024-03-15 12:15:39,156 - 
2024-03-15 12:16:04,762 - Epoch: 143, Steps: 235 | Lr: 9.78058e-05 | Train Loss: 1.62569e-03
2024-03-15 12:16:04,762 - Epoch time: 25.61s, Average time: 25.77s
2024-03-15 12:16:05,077 - 
2024-03-15 12:16:30,699 - Epoch: 144, Steps: 235 | Lr: 9.80815e-05 | Train Loss: 1.54712e-03
2024-03-15 12:16:30,700 - Epoch time: 25.62s, Average time: 25.77s
2024-03-15 12:16:30,700 - Loss decreased (1.56793e-03 --> 1.54712e-03).  Saving best model ...
2024-03-15 12:16:31,127 - 
2024-03-15 12:16:56,612 - Epoch: 145, Steps: 235 | Lr: 9.83391e-05 | Train Loss: 1.53511e-03
2024-03-15 12:16:56,613 - Epoch time: 25.48s, Average time: 25.76s
2024-03-15 12:16:56,613 - Loss decreased (1.54712e-03 --> 1.53511e-03).  Saving best model ...
2024-03-15 12:16:57,064 - 
2024-03-15 12:17:22,405 - Epoch: 146, Steps: 235 | Lr: 9.85784e-05 | Train Loss: 1.47746e-03
2024-03-15 12:17:22,406 - Epoch time: 25.34s, Average time: 25.76s
2024-03-15 12:17:22,406 - Loss decreased (1.53511e-03 --> 1.47746e-03).  Saving best model ...
2024-03-15 12:17:22,840 - 
2024-03-15 12:17:48,443 - Epoch: 147, Steps: 235 | Lr: 9.87993e-05 | Train Loss: 1.47121e-03
2024-03-15 12:17:48,443 - Epoch time: 25.60s, Average time: 25.76s
2024-03-15 12:17:48,443 - Loss decreased (1.47746e-03 --> 1.47121e-03).  Saving best model ...
2024-03-15 12:17:48,863 - 
2024-03-15 12:18:14,331 - Epoch: 148, Steps: 235 | Lr: 9.90018e-05 | Train Loss: 1.42643e-03
2024-03-15 12:18:14,332 - Epoch time: 25.47s, Average time: 25.76s
2024-03-15 12:18:14,332 - Loss decreased (1.47121e-03 --> 1.42643e-03).  Saving best model ...
2024-03-15 12:18:14,752 - 
2024-03-15 12:18:40,246 - Epoch: 149, Steps: 235 | Lr: 9.91858e-05 | Train Loss: 1.49655e-03
2024-03-15 12:18:40,246 - Epoch time: 25.49s, Average time: 25.76s
2024-03-15 12:18:40,579 - 
2024-03-15 12:19:06,136 - Epoch: 150, Steps: 235 | Lr: 9.93512e-05 | Train Loss: 1.47704e-03
2024-03-15 12:19:06,137 - Epoch time: 25.56s, Average time: 25.75s
2024-03-15 12:19:06,142 - Save the loss_recoder of 150 epochs
2024-03-15 12:19:06,163 - Simulation picture saved in /home/bingxing2/home/scx6mfb/mhy/SAU/CFD-CNN/Model/SAU_1001_10to1-0.3/pic/loss_record.png
2024-03-15 12:19:07,119 - 
2024-03-15 12:19:32,584 - Epoch: 151, Steps: 235 | Lr: 9.94980e-05 | Train Loss: 1.49530e-03
2024-03-15 12:19:32,585 - Epoch time: 25.46s, Average time: 25.75s
2024-03-15 12:19:32,913 - 
2024-03-15 12:19:58,316 - Epoch: 152, Steps: 235 | Lr: 9.96260e-05 | Train Loss: 1.38736e-03
2024-03-15 12:19:58,316 - Epoch time: 25.40s, Average time: 25.75s
2024-03-15 12:19:58,316 - Loss decreased (1.42643e-03 --> 1.38736e-03).  Saving best model ...
2024-03-15 12:19:58,752 - 
2024-03-15 12:20:24,087 - Epoch: 153, Steps: 235 | Lr: 9.97353e-05 | Train Loss: 1.27153e-03
2024-03-15 12:20:24,087 - Epoch time: 25.33s, Average time: 25.75s
2024-03-15 12:20:24,088 - Loss decreased (1.38736e-03 --> 1.27153e-03).  Saving best model ...
2024-03-15 12:20:24,504 - 
2024-03-15 12:20:50,261 - Epoch: 154, Steps: 235 | Lr: 9.98258e-05 | Train Loss: 1.37909e-03
2024-03-15 12:20:50,261 - Epoch time: 25.76s, Average time: 25.75s
2024-03-15 12:20:50,585 - 
2024-03-15 12:21:15,938 - Epoch: 155, Steps: 235 | Lr: 9.98975e-05 | Train Loss: 1.26638e-03
2024-03-15 12:21:15,938 - Epoch time: 25.35s, Average time: 25.75s
2024-03-15 12:21:15,938 - Loss decreased (1.27153e-03 --> 1.26638e-03).  Saving best model ...
2024-03-15 12:21:16,380 - 
2024-03-15 12:21:41,781 - Epoch: 156, Steps: 235 | Lr: 9.99503e-05 | Train Loss: 1.39191e-03
2024-03-15 12:21:41,781 - Epoch time: 25.40s, Average time: 25.74s
2024-03-15 12:21:42,210 - 
2024-03-15 12:22:07,767 - Epoch: 157, Steps: 235 | Lr: 9.99842e-05 | Train Loss: 1.28896e-03
2024-03-15 12:22:07,767 - Epoch time: 25.56s, Average time: 25.74s
2024-03-15 12:22:08,094 - 
2024-03-15 12:22:33,522 - Epoch: 158, Steps: 235 | Lr: 9.99992e-05 | Train Loss: 1.23742e-03
2024-03-15 12:22:33,523 - Epoch time: 25.43s, Average time: 25.74s
2024-03-15 12:22:33,523 - Loss decreased (1.26638e-03 --> 1.23742e-03).  Saving best model ...
2024-03-15 12:22:33,939 - 
2024-03-15 12:22:59,355 - Epoch: 159, Steps: 235 | Lr: 9.99991e-05 | Train Loss: 1.25899e-03
2024-03-15 12:22:59,355 - Epoch time: 25.42s, Average time: 25.74s
2024-03-15 12:22:59,666 - 
2024-03-15 12:23:24,997 - Epoch: 160, Steps: 235 | Lr: 9.99947e-05 | Train Loss: 1.25876e-03
2024-03-15 12:23:24,997 - Epoch time: 25.33s, Average time: 25.74s
2024-03-15 12:23:25,311 - 
2024-03-15 12:23:50,914 - Epoch: 161, Steps: 235 | Lr: 9.99868e-05 | Train Loss: 1.21561e-03
2024-03-15 12:23:50,914 - Epoch time: 25.60s, Average time: 25.73s
2024-03-15 12:23:50,915 - Loss decreased (1.23742e-03 --> 1.21561e-03).  Saving best model ...
2024-03-15 12:23:51,336 - 
2024-03-15 12:24:16,714 - Epoch: 162, Steps: 235 | Lr: 9.99752e-05 | Train Loss: 1.13983e-03
2024-03-15 12:24:16,714 - Epoch time: 25.38s, Average time: 25.73s
2024-03-15 12:24:16,714 - Loss decreased (1.21561e-03 --> 1.13983e-03).  Saving best model ...
2024-03-15 12:24:17,158 - 
2024-03-15 12:24:42,704 - Epoch: 163, Steps: 235 | Lr: 9.99599e-05 | Train Loss: 1.27365e-03
2024-03-15 12:24:42,704 - Epoch time: 25.54s, Average time: 25.73s
2024-03-15 12:24:43,028 - 
2024-03-15 12:25:08,568 - Epoch: 164, Steps: 235 | Lr: 9.99411e-05 | Train Loss: 1.12398e-03
2024-03-15 12:25:08,568 - Epoch time: 25.54s, Average time: 25.73s
2024-03-15 12:25:08,569 - Loss decreased (1.13983e-03 --> 1.12398e-03).  Saving best model ...
2024-03-15 12:25:08,981 - 
2024-03-15 12:25:34,126 - Epoch: 165, Steps: 235 | Lr: 9.99187e-05 | Train Loss: 1.11255e-03
2024-03-15 12:25:34,127 - Epoch time: 25.15s, Average time: 25.73s
2024-03-15 12:25:34,127 - Loss decreased (1.12398e-03 --> 1.11255e-03).  Saving best model ...
2024-03-15 12:25:34,558 - 
2024-03-15 12:25:59,869 - Epoch: 166, Steps: 235 | Lr: 9.98926e-05 | Train Loss: 1.18999e-03
2024-03-15 12:25:59,869 - Epoch time: 25.31s, Average time: 25.72s
2024-03-15 12:26:00,207 - 
2024-03-15 12:26:25,877 - Epoch: 167, Steps: 235 | Lr: 9.98630e-05 | Train Loss: 1.15448e-03
2024-03-15 12:26:25,878 - Epoch time: 25.67s, Average time: 25.72s
2024-03-15 12:26:26,197 - 
2024-03-15 12:26:51,811 - Epoch: 168, Steps: 235 | Lr: 9.98297e-05 | Train Loss: 1.04625e-03
2024-03-15 12:26:51,811 - Epoch time: 25.61s, Average time: 25.72s
2024-03-15 12:26:51,811 - Loss decreased (1.11255e-03 --> 1.04625e-03).  Saving best model ...
2024-03-15 12:26:52,234 - 
2024-03-15 12:27:17,636 - Epoch: 169, Steps: 235 | Lr: 9.97928e-05 | Train Loss: 1.07065e-03
2024-03-15 12:27:17,637 - Epoch time: 25.40s, Average time: 25.72s
2024-03-15 12:27:18,057 - 
2024-03-15 12:27:43,277 - Epoch: 170, Steps: 235 | Lr: 9.97524e-05 | Train Loss: 1.06164e-03
2024-03-15 12:27:43,278 - Epoch time: 25.22s, Average time: 25.72s
2024-03-15 12:27:43,609 - 
2024-03-15 12:28:09,067 - Epoch: 171, Steps: 235 | Lr: 9.97083e-05 | Train Loss: 1.07599e-03
2024-03-15 12:28:09,068 - Epoch time: 25.46s, Average time: 25.72s
2024-03-15 12:28:09,380 - 
2024-03-15 12:28:34,809 - Epoch: 172, Steps: 235 | Lr: 9.96606e-05 | Train Loss: 1.00251e-03
2024-03-15 12:28:34,810 - Epoch time: 25.43s, Average time: 25.71s
2024-03-15 12:28:34,810 - Loss decreased (1.04625e-03 --> 1.00251e-03).  Saving best model ...
2024-03-15 12:28:35,228 - 
2024-03-15 12:29:00,612 - Epoch: 173, Steps: 235 | Lr: 9.96094e-05 | Train Loss: 1.00728e-03
2024-03-15 12:29:00,612 - Epoch time: 25.38s, Average time: 25.71s
2024-03-15 12:29:00,942 - 
2024-03-15 12:29:26,400 - Epoch: 174, Steps: 235 | Lr: 9.95545e-05 | Train Loss: 1.01567e-03
2024-03-15 12:29:26,400 - Epoch time: 25.46s, Average time: 25.71s
2024-03-15 12:29:26,724 - 
2024-03-15 12:29:52,127 - Epoch: 175, Steps: 235 | Lr: 9.94961e-05 | Train Loss: 1.01303e-03
2024-03-15 12:29:52,127 - Epoch time: 25.40s, Average time: 25.71s
2024-03-15 12:29:52,452 - 
2024-03-15 12:30:17,945 - Epoch: 176, Steps: 235 | Lr: 9.94341e-05 | Train Loss: 9.95051e-04
2024-03-15 12:30:17,945 - Epoch time: 25.49s, Average time: 25.71s
2024-03-15 12:30:17,946 - Loss decreased (1.00251e-03 --> 9.95051e-04).  Saving best model ...
2024-03-15 12:30:18,382 - 
2024-03-15 12:30:44,031 - Epoch: 177, Steps: 235 | Lr: 9.93685e-05 | Train Loss: 9.72175e-04
2024-03-15 12:30:44,032 - Epoch time: 25.65s, Average time: 25.71s
2024-03-15 12:30:44,032 - Loss decreased (9.95051e-04 --> 9.72175e-04).  Saving best model ...
2024-03-15 12:30:44,451 - 
2024-03-15 12:31:10,065 - Epoch: 178, Steps: 235 | Lr: 9.92993e-05 | Train Loss: 9.43210e-04
2024-03-15 12:31:10,066 - Epoch time: 25.61s, Average time: 25.71s
2024-03-15 12:31:10,066 - Loss decreased (9.72175e-04 --> 9.43210e-04).  Saving best model ...
2024-03-15 12:31:10,479 - 
2024-03-15 12:31:36,558 - Epoch: 179, Steps: 235 | Lr: 9.92266e-05 | Train Loss: 1.05728e-03
2024-03-15 12:31:36,558 - Epoch time: 26.08s, Average time: 25.71s
2024-03-15 12:31:36,869 - 
2024-03-15 12:32:02,235 - Epoch: 180, Steps: 235 | Lr: 9.91503e-05 | Train Loss: 8.99503e-04
2024-03-15 12:32:02,235 - Epoch time: 25.37s, Average time: 25.71s
2024-03-15 12:32:02,236 - Loss decreased (9.43210e-04 --> 8.99503e-04).  Saving best model ...
2024-03-15 12:32:02,666 - 
2024-03-15 12:32:28,082 - Epoch: 181, Steps: 235 | Lr: 9.90704e-05 | Train Loss: 8.72816e-04
2024-03-15 12:32:28,083 - Epoch time: 25.42s, Average time: 25.71s
2024-03-15 12:32:28,083 - Loss decreased (8.99503e-04 --> 8.72816e-04).  Saving best model ...
2024-03-15 12:32:28,522 - 
2024-03-15 12:32:53,920 - Epoch: 182, Steps: 235 | Lr: 9.89870e-05 | Train Loss: 9.35312e-04
2024-03-15 12:32:53,920 - Epoch time: 25.40s, Average time: 25.70s
2024-03-15 12:32:54,233 - 
2024-03-15 12:33:19,489 - Epoch: 183, Steps: 235 | Lr: 9.89001e-05 | Train Loss: 8.95833e-04
2024-03-15 12:33:19,490 - Epoch time: 25.26s, Average time: 25.70s
2024-03-15 12:33:19,807 - 
2024-03-15 12:33:45,036 - Epoch: 184, Steps: 235 | Lr: 9.88096e-05 | Train Loss: 8.60595e-04
2024-03-15 12:33:45,036 - Epoch time: 25.23s, Average time: 25.70s
2024-03-15 12:33:45,037 - Loss decreased (8.72816e-04 --> 8.60595e-04).  Saving best model ...
2024-03-15 12:33:45,459 - 
2024-03-15 12:34:10,724 - Epoch: 185, Steps: 235 | Lr: 9.87156e-05 | Train Loss: 9.40701e-04
2024-03-15 12:34:10,725 - Epoch time: 25.27s, Average time: 25.70s
2024-03-15 12:34:11,032 - 
2024-03-15 12:34:36,621 - Epoch: 186, Steps: 235 | Lr: 9.86181e-05 | Train Loss: 8.56382e-04
2024-03-15 12:34:36,621 - Epoch time: 25.59s, Average time: 25.70s
2024-03-15 12:34:36,622 - Loss decreased (8.60595e-04 --> 8.56382e-04).  Saving best model ...
2024-03-15 12:34:37,040 - 
2024-03-15 12:35:02,432 - Epoch: 187, Steps: 235 | Lr: 9.85170e-05 | Train Loss: 8.29477e-04
2024-03-15 12:35:02,433 - Epoch time: 25.39s, Average time: 25.69s
2024-03-15 12:35:02,433 - Loss decreased (8.56382e-04 --> 8.29477e-04).  Saving best model ...
2024-03-15 12:35:02,957 - 
2024-03-15 12:35:28,412 - Epoch: 188, Steps: 235 | Lr: 9.84125e-05 | Train Loss: 8.50626e-04
2024-03-15 12:35:28,412 - Epoch time: 25.45s, Average time: 25.69s
2024-03-15 12:35:28,735 - 
2024-03-15 12:35:54,376 - Epoch: 189, Steps: 235 | Lr: 9.83044e-05 | Train Loss: 8.54980e-04
2024-03-15 12:35:54,376 - Epoch time: 25.64s, Average time: 25.69s
2024-03-15 12:35:54,690 - 
2024-03-15 12:36:19,962 - Epoch: 190, Steps: 235 | Lr: 9.81929e-05 | Train Loss: 8.47353e-04
2024-03-15 12:36:19,962 - Epoch time: 25.27s, Average time: 25.69s
2024-03-15 12:36:20,285 - 
2024-03-15 12:36:45,995 - Epoch: 191, Steps: 235 | Lr: 9.80778e-05 | Train Loss: 8.03178e-04
2024-03-15 12:36:45,995 - Epoch time: 25.71s, Average time: 25.69s
2024-03-15 12:36:45,996 - Loss decreased (8.29477e-04 --> 8.03178e-04).  Saving best model ...
2024-03-15 12:36:46,406 - 
2024-03-15 12:37:11,883 - Epoch: 192, Steps: 235 | Lr: 9.79593e-05 | Train Loss: 8.07934e-04
2024-03-15 12:37:11,883 - Epoch time: 25.48s, Average time: 25.69s
2024-03-15 12:37:12,201 - 
2024-03-15 12:37:37,389 - Epoch: 193, Steps: 235 | Lr: 9.78373e-05 | Train Loss: 7.73196e-04
2024-03-15 12:37:37,390 - Epoch time: 25.19s, Average time: 25.69s
2024-03-15 12:37:37,390 - Loss decreased (8.03178e-04 --> 7.73196e-04).  Saving best model ...
2024-03-15 12:37:37,804 - 
2024-03-15 12:38:03,783 - Epoch: 194, Steps: 235 | Lr: 9.77118e-05 | Train Loss: 7.66545e-04
2024-03-15 12:38:03,784 - Epoch time: 25.98s, Average time: 25.69s
2024-03-15 12:38:03,784 - Loss decreased (7.73196e-04 --> 7.66545e-04).  Saving best model ...
2024-03-15 12:38:04,205 - 
2024-03-15 12:38:29,612 - Epoch: 195, Steps: 235 | Lr: 9.75829e-05 | Train Loss: 7.95556e-04
2024-03-15 12:38:29,612 - Epoch time: 25.41s, Average time: 25.69s
2024-03-15 12:38:29,927 - 
2024-03-15 12:38:55,248 - Epoch: 196, Steps: 235 | Lr: 9.74506e-05 | Train Loss: 8.68202e-04
2024-03-15 12:38:55,248 - Epoch time: 25.32s, Average time: 25.69s
2024-03-15 12:38:55,559 - 
2024-03-15 12:39:20,882 - Epoch: 197, Steps: 235 | Lr: 9.73148e-05 | Train Loss: 7.82419e-04
2024-03-15 12:39:20,882 - Epoch time: 25.32s, Average time: 25.68s
2024-03-15 12:39:21,205 - 
2024-03-15 12:39:46,719 - Epoch: 198, Steps: 235 | Lr: 9.71756e-05 | Train Loss: 7.55236e-04
2024-03-15 12:39:46,720 - Epoch time: 25.51s, Average time: 25.68s
2024-03-15 12:39:46,720 - Loss decreased (7.66545e-04 --> 7.55236e-04).  Saving best model ...
2024-03-15 12:39:47,165 - 
2024-03-15 12:40:12,803 - Epoch: 199, Steps: 235 | Lr: 9.70330e-05 | Train Loss: 6.99450e-04
2024-03-15 12:40:12,804 - Epoch time: 25.64s, Average time: 25.68s
2024-03-15 12:40:12,804 - Loss decreased (7.55236e-04 --> 6.99450e-04).  Saving best model ...
2024-03-15 12:40:13,222 - 
2024-03-15 12:40:38,615 - Epoch: 200, Steps: 235 | Lr: 9.68870e-05 | Train Loss: 7.32868e-04
2024-03-15 12:40:38,615 - Epoch time: 25.39s, Average time: 25.68s
2024-03-15 12:40:38,621 - Save the loss_recoder of 200 epochs
2024-03-15 12:40:38,642 - Simulation picture saved in /home/bingxing2/home/scx6mfb/mhy/SAU/CFD-CNN/Model/SAU_1001_10to1-0.3/pic/loss_record.png
2024-03-15 12:40:39,662 - 
2024-03-15 12:41:05,022 - Epoch: 201, Steps: 235 | Lr: 9.67376e-05 | Train Loss: 7.64073e-04
2024-03-15 12:41:05,022 - Epoch time: 25.36s, Average time: 25.68s
2024-03-15 12:41:05,333 - 
2024-03-15 12:41:30,800 - Epoch: 202, Steps: 235 | Lr: 9.65848e-05 | Train Loss: 7.07641e-04
2024-03-15 12:41:30,800 - Epoch time: 25.47s, Average time: 25.68s
2024-03-15 12:41:31,128 - 
2024-03-15 12:41:56,589 - Epoch: 203, Steps: 235 | Lr: 9.64286e-05 | Train Loss: 7.11311e-04
2024-03-15 12:41:56,589 - Epoch time: 25.46s, Average time: 25.68s
2024-03-15 12:41:56,908 - 
2024-03-15 12:42:22,329 - Epoch: 204, Steps: 235 | Lr: 9.62691e-05 | Train Loss: 7.27217e-04
2024-03-15 12:42:22,330 - Epoch time: 25.42s, Average time: 25.68s
2024-03-15 12:42:22,644 - 
2024-03-15 12:42:48,154 - Epoch: 205, Steps: 235 | Lr: 9.61063e-05 | Train Loss: 7.19264e-04
2024-03-15 12:42:48,154 - Epoch time: 25.51s, Average time: 25.68s
2024-03-15 12:42:48,465 - 
2024-03-15 12:43:13,904 - Epoch: 206, Steps: 235 | Lr: 9.59401e-05 | Train Loss: 6.94245e-04
2024-03-15 12:43:13,905 - Epoch time: 25.44s, Average time: 25.67s
2024-03-15 12:43:13,905 - Loss decreased (6.99450e-04 --> 6.94245e-04).  Saving best model ...
2024-03-15 12:43:14,327 - 
2024-03-15 12:43:39,792 - Epoch: 207, Steps: 235 | Lr: 9.57705e-05 | Train Loss: 6.94168e-04
2024-03-15 12:43:39,792 - Epoch time: 25.46s, Average time: 25.67s
2024-03-15 12:43:39,793 - Loss decreased (6.94245e-04 --> 6.94168e-04).  Saving best model ...
2024-03-15 12:43:40,208 - 
2024-03-15 12:44:05,652 - Epoch: 208, Steps: 235 | Lr: 9.55977e-05 | Train Loss: 7.58944e-04
2024-03-15 12:44:05,653 - Epoch time: 25.44s, Average time: 25.67s
2024-03-15 12:44:05,965 - 
2024-03-15 12:44:31,307 - Epoch: 209, Steps: 235 | Lr: 9.54216e-05 | Train Loss: 6.59750e-04
2024-03-15 12:44:31,308 - Epoch time: 25.34s, Average time: 25.67s
2024-03-15 12:44:31,308 - Loss decreased (6.94168e-04 --> 6.59750e-04).  Saving best model ...
2024-03-15 12:44:31,741 - 
2024-03-15 12:44:57,250 - Epoch: 210, Steps: 235 | Lr: 9.52421e-05 | Train Loss: 6.46318e-04
2024-03-15 12:44:57,250 - Epoch time: 25.51s, Average time: 25.67s
2024-03-15 12:44:57,250 - Loss decreased (6.59750e-04 --> 6.46318e-04).  Saving best model ...
2024-03-15 12:44:57,670 - 
2024-03-15 12:45:23,140 - Epoch: 211, Steps: 235 | Lr: 9.50595e-05 | Train Loss: 6.71742e-04
2024-03-15 12:45:23,140 - Epoch time: 25.47s, Average time: 25.67s
2024-03-15 12:45:23,472 - 
2024-03-15 12:45:48,956 - Epoch: 212, Steps: 235 | Lr: 9.48735e-05 | Train Loss: 7.02651e-04
2024-03-15 12:45:48,956 - Epoch time: 25.48s, Average time: 25.67s
2024-03-15 12:45:49,266 - 
2024-03-15 12:46:14,838 - Epoch: 213, Steps: 235 | Lr: 9.46843e-05 | Train Loss: 6.39966e-04
2024-03-15 12:46:14,838 - Epoch time: 25.57s, Average time: 25.67s
2024-03-15 12:46:14,839 - Loss decreased (6.46318e-04 --> 6.39966e-04).  Saving best model ...
2024-03-15 12:46:15,277 - 
2024-03-15 12:46:40,659 - Epoch: 214, Steps: 235 | Lr: 9.44919e-05 | Train Loss: 6.45024e-04
2024-03-15 12:46:40,659 - Epoch time: 25.38s, Average time: 25.67s
2024-03-15 12:46:40,981 - 
2024-03-15 12:47:06,609 - Epoch: 215, Steps: 235 | Lr: 9.42962e-05 | Train Loss: 6.38272e-04
2024-03-15 12:47:06,609 - Epoch time: 25.63s, Average time: 25.67s
2024-03-15 12:47:06,609 - Loss decreased (6.39966e-04 --> 6.38272e-04).  Saving best model ...
2024-03-15 12:47:07,034 - 
2024-03-15 12:47:32,521 - Epoch: 216, Steps: 235 | Lr: 9.40974e-05 | Train Loss: 6.29471e-04
2024-03-15 12:47:32,521 - Epoch time: 25.49s, Average time: 25.67s
2024-03-15 12:47:32,521 - Loss decreased (6.38272e-04 --> 6.29471e-04).  Saving best model ...
2024-03-15 12:47:32,970 - 
2024-03-15 12:47:58,740 - Epoch: 217, Steps: 235 | Lr: 9.38953e-05 | Train Loss: 6.49622e-04
2024-03-15 12:47:58,740 - Epoch time: 25.77s, Average time: 25.67s
2024-03-15 12:47:59,060 - 
2024-03-15 12:48:24,377 - Epoch: 218, Steps: 235 | Lr: 9.36901e-05 | Train Loss: 6.32343e-04
2024-03-15 12:48:24,377 - Epoch time: 25.32s, Average time: 25.66s
2024-03-15 12:48:24,722 - 
2024-03-15 12:48:50,271 - Epoch: 219, Steps: 235 | Lr: 9.34817e-05 | Train Loss: 6.13408e-04
2024-03-15 12:48:50,271 - Epoch time: 25.55s, Average time: 25.66s
2024-03-15 12:48:50,271 - Loss decreased (6.29471e-04 --> 6.13408e-04).  Saving best model ...
2024-03-15 12:48:50,706 - 
2024-03-15 12:49:16,178 - Epoch: 220, Steps: 235 | Lr: 9.32702e-05 | Train Loss: 6.29346e-04
2024-03-15 12:49:16,179 - Epoch time: 25.47s, Average time: 25.66s
2024-03-15 12:49:16,496 - 
2024-03-15 12:49:41,974 - Epoch: 221, Steps: 235 | Lr: 9.30555e-05 | Train Loss: 5.99292e-04
2024-03-15 12:49:41,975 - Epoch time: 25.48s, Average time: 25.66s
2024-03-15 12:49:41,975 - Loss decreased (6.13408e-04 --> 5.99292e-04).  Saving best model ...
2024-03-15 12:49:42,392 - 
2024-03-15 12:50:07,834 - Epoch: 222, Steps: 235 | Lr: 9.28378e-05 | Train Loss: 5.95490e-04
2024-03-15 12:50:07,835 - Epoch time: 25.44s, Average time: 25.66s
2024-03-15 12:50:07,835 - Loss decreased (5.99292e-04 --> 5.95490e-04).  Saving best model ...
2024-03-15 12:50:08,277 - 
2024-03-15 12:50:33,722 - Epoch: 223, Steps: 235 | Lr: 9.26169e-05 | Train Loss: 5.93566e-04
2024-03-15 12:50:33,722 - Epoch time: 25.44s, Average time: 25.66s
2024-03-15 12:50:33,722 - Loss decreased (5.95490e-04 --> 5.93566e-04).  Saving best model ...
2024-03-15 12:50:34,144 - 
2024-03-15 12:50:59,789 - Epoch: 224, Steps: 235 | Lr: 9.23929e-05 | Train Loss: 6.37795e-04
2024-03-15 12:50:59,790 - Epoch time: 25.64s, Average time: 25.66s
2024-03-15 12:51:00,112 - 
2024-03-15 12:51:25,606 - Epoch: 225, Steps: 235 | Lr: 9.21659e-05 | Train Loss: 6.22533e-04
2024-03-15 12:51:25,606 - Epoch time: 25.49s, Average time: 25.66s
2024-03-15 12:51:25,917 - 
2024-03-15 12:51:51,387 - Epoch: 226, Steps: 235 | Lr: 9.19359e-05 | Train Loss: 5.90498e-04
2024-03-15 12:51:51,387 - Epoch time: 25.47s, Average time: 25.66s
2024-03-15 12:51:51,388 - Loss decreased (5.93566e-04 --> 5.90498e-04).  Saving best model ...
2024-03-15 12:51:51,791 - 
2024-03-15 12:52:17,444 - Epoch: 227, Steps: 235 | Lr: 9.17028e-05 | Train Loss: 5.73842e-04
2024-03-15 12:52:17,444 - Epoch time: 25.65s, Average time: 25.66s
2024-03-15 12:52:17,444 - Loss decreased (5.90498e-04 --> 5.73842e-04).  Saving best model ...
2024-03-15 12:52:17,866 - 
2024-03-15 12:52:43,451 - Epoch: 228, Steps: 235 | Lr: 9.14666e-05 | Train Loss: 5.68344e-04
2024-03-15 12:52:43,452 - Epoch time: 25.58s, Average time: 25.66s
2024-03-15 12:52:43,452 - Loss decreased (5.73842e-04 --> 5.68344e-04).  Saving best model ...
2024-03-15 12:52:43,895 - 
2024-03-15 12:53:09,390 - Epoch: 229, Steps: 235 | Lr: 9.12275e-05 | Train Loss: 5.95193e-04
2024-03-15 12:53:09,390 - Epoch time: 25.49s, Average time: 25.66s
2024-03-15 12:53:09,717 - 
2024-03-15 12:53:35,295 - Epoch: 230, Steps: 235 | Lr: 9.09854e-05 | Train Loss: 5.45559e-04
2024-03-15 12:53:35,295 - Epoch time: 25.58s, Average time: 25.66s
2024-03-15 12:53:35,296 - Loss decreased (5.68344e-04 --> 5.45559e-04).  Saving best model ...
2024-03-15 12:53:35,725 - 
2024-03-15 12:54:01,135 - Epoch: 231, Steps: 235 | Lr: 9.07403e-05 | Train Loss: 6.15635e-04
2024-03-15 12:54:01,136 - Epoch time: 25.41s, Average time: 25.66s
2024-03-15 12:54:01,452 - 
2024-03-15 12:54:26,971 - Epoch: 232, Steps: 235 | Lr: 9.04923e-05 | Train Loss: 5.63297e-04
2024-03-15 12:54:26,971 - Epoch time: 25.52s, Average time: 25.66s
2024-03-15 12:54:27,291 - 
2024-03-15 12:54:53,023 - Epoch: 233, Steps: 235 | Lr: 9.02414e-05 | Train Loss: 5.34293e-04
2024-03-15 12:54:53,024 - Epoch time: 25.73s, Average time: 25.66s
2024-03-15 12:54:53,024 - Loss decreased (5.45559e-04 --> 5.34293e-04).  Saving best model ...
2024-03-15 12:54:53,479 - 
2024-03-15 12:55:19,020 - Epoch: 234, Steps: 235 | Lr: 8.99875e-05 | Train Loss: 5.38163e-04
2024-03-15 12:55:19,020 - Epoch time: 25.54s, Average time: 25.65s
2024-03-15 12:55:19,369 - 
2024-03-15 12:55:44,913 - Epoch: 235, Steps: 235 | Lr: 8.97308e-05 | Train Loss: 5.64032e-04
2024-03-15 12:55:44,913 - Epoch time: 25.54s, Average time: 25.65s
2024-03-15 12:55:45,250 - 
2024-03-15 12:56:10,895 - Epoch: 236, Steps: 235 | Lr: 8.94712e-05 | Train Loss: 5.33325e-04
2024-03-15 12:56:10,896 - Epoch time: 25.65s, Average time: 25.65s
2024-03-15 12:56:10,896 - Loss decreased (5.34293e-04 --> 5.33325e-04).  Saving best model ...
2024-03-15 12:56:11,342 - 
2024-03-15 12:56:36,820 - Epoch: 237, Steps: 235 | Lr: 8.92087e-05 | Train Loss: 5.20114e-04
2024-03-15 12:56:36,820 - Epoch time: 25.48s, Average time: 25.65s
2024-03-15 12:56:36,820 - Loss decreased (5.33325e-04 --> 5.20114e-04).  Saving best model ...
2024-03-15 12:56:37,236 - 
2024-03-15 12:57:02,773 - Epoch: 238, Steps: 235 | Lr: 8.89434e-05 | Train Loss: 5.53377e-04
2024-03-15 12:57:02,774 - Epoch time: 25.54s, Average time: 25.65s
2024-03-15 12:57:03,087 - 
2024-03-15 12:57:28,541 - Epoch: 239, Steps: 235 | Lr: 8.86752e-05 | Train Loss: 5.14660e-04
2024-03-15 12:57:28,541 - Epoch time: 25.45s, Average time: 25.65s
2024-03-15 12:57:28,542 - Loss decreased (5.20114e-04 --> 5.14660e-04).  Saving best model ...
2024-03-15 12:57:28,981 - 
2024-03-15 12:57:54,500 - Epoch: 240, Steps: 235 | Lr: 8.84043e-05 | Train Loss: 5.41201e-04
2024-03-15 12:57:54,500 - Epoch time: 25.52s, Average time: 25.65s
2024-03-15 12:57:54,815 - 
2024-03-15 12:58:20,414 - Epoch: 241, Steps: 235 | Lr: 8.81306e-05 | Train Loss: 5.60584e-04
2024-03-15 12:58:20,414 - Epoch time: 25.60s, Average time: 25.65s
2024-03-15 12:58:20,723 - 
2024-03-15 12:58:46,186 - Epoch: 242, Steps: 235 | Lr: 8.78541e-05 | Train Loss: 5.05920e-04
2024-03-15 12:58:46,187 - Epoch time: 25.46s, Average time: 25.65s
2024-03-15 12:58:46,187 - Loss decreased (5.14660e-04 --> 5.05920e-04).  Saving best model ...
2024-03-15 12:58:46,601 - 
2024-03-15 12:59:12,042 - Epoch: 243, Steps: 235 | Lr: 8.75749e-05 | Train Loss: 5.19212e-04
2024-03-15 12:59:12,042 - Epoch time: 25.44s, Average time: 25.65s
2024-03-15 12:59:12,363 - 
2024-03-15 12:59:37,946 - Epoch: 244, Steps: 235 | Lr: 8.72930e-05 | Train Loss: 5.39241e-04
2024-03-15 12:59:37,946 - Epoch time: 25.58s, Average time: 25.65s
2024-03-15 12:59:38,277 - 
2024-03-15 13:00:03,799 - Epoch: 245, Steps: 235 | Lr: 8.70084e-05 | Train Loss: 4.94696e-04
2024-03-15 13:00:03,800 - Epoch time: 25.52s, Average time: 25.65s
2024-03-15 13:00:03,800 - Loss decreased (5.05920e-04 --> 4.94696e-04).  Saving best model ...
2024-03-15 13:00:04,239 - 
2024-03-15 13:00:29,763 - Epoch: 246, Steps: 235 | Lr: 8.67211e-05 | Train Loss: 4.79561e-04
2024-03-15 13:00:29,763 - Epoch time: 25.52s, Average time: 25.65s
2024-03-15 13:00:29,763 - Loss decreased (4.94696e-04 --> 4.79561e-04).  Saving best model ...
2024-03-15 13:00:30,194 - 
2024-03-15 13:00:55,804 - Epoch: 247, Steps: 235 | Lr: 8.64311e-05 | Train Loss: 4.85530e-04
2024-03-15 13:00:55,804 - Epoch time: 25.61s, Average time: 25.65s
2024-03-15 13:00:56,136 - 
2024-03-15 13:01:21,616 - Epoch: 248, Steps: 235 | Lr: 8.61385e-05 | Train Loss: 5.04113e-04
2024-03-15 13:01:21,616 - Epoch time: 25.48s, Average time: 25.65s
2024-03-15 13:01:21,936 - 
2024-03-15 13:01:47,413 - Epoch: 249, Steps: 235 | Lr: 8.58433e-05 | Train Loss: 4.96227e-04
2024-03-15 13:01:47,413 - Epoch time: 25.48s, Average time: 25.65s
2024-03-15 13:01:47,748 - 
2024-03-15 13:02:13,100 - Epoch: 250, Steps: 235 | Lr: 8.55455e-05 | Train Loss: 4.89828e-04
2024-03-15 13:02:13,101 - Epoch time: 25.35s, Average time: 25.65s
2024-03-15 13:02:13,106 - Save the loss_recoder of 250 epochs
2024-03-15 13:02:13,126 - Simulation picture saved in /home/bingxing2/home/scx6mfb/mhy/SAU/CFD-CNN/Model/SAU_1001_10to1-0.3/pic/loss_record.png
2024-03-15 13:02:14,249 - 
2024-03-15 13:02:39,744 - Epoch: 251, Steps: 235 | Lr: 8.52452e-05 | Train Loss: 4.96786e-04
2024-03-15 13:02:39,744 - Epoch time: 25.49s, Average time: 25.65s
2024-03-15 13:02:40,070 - 
2024-03-15 13:03:05,773 - Epoch: 252, Steps: 235 | Lr: 8.49423e-05 | Train Loss: 4.85658e-04
2024-03-15 13:03:05,773 - Epoch time: 25.70s, Average time: 25.65s
2024-03-15 13:03:06,166 - 
2024-03-15 13:03:31,792 - Epoch: 253, Steps: 235 | Lr: 8.46368e-05 | Train Loss: 4.86831e-04
2024-03-15 13:03:31,793 - Epoch time: 25.63s, Average time: 25.65s
2024-03-15 13:03:32,104 - 
2024-03-15 13:03:57,543 - Epoch: 254, Steps: 235 | Lr: 8.43289e-05 | Train Loss: 4.69700e-04
2024-03-15 13:03:57,543 - Epoch time: 25.44s, Average time: 25.64s
2024-03-15 13:03:57,544 - Loss decreased (4.79561e-04 --> 4.69700e-04).  Saving best model ...
2024-03-15 13:03:57,996 - 
2024-03-15 13:04:23,502 - Epoch: 255, Steps: 235 | Lr: 8.40184e-05 | Train Loss: 4.93362e-04
2024-03-15 13:04:23,502 - Epoch time: 25.50s, Average time: 25.64s
2024-03-15 13:04:23,831 - 
2024-03-15 13:04:49,148 - Epoch: 256, Steps: 235 | Lr: 8.37055e-05 | Train Loss: 4.84495e-04
2024-03-15 13:04:49,148 - Epoch time: 25.32s, Average time: 25.64s
2024-03-15 13:04:49,467 - 
2024-03-15 13:05:14,955 - Epoch: 257, Steps: 235 | Lr: 8.33902e-05 | Train Loss: 4.57020e-04
2024-03-15 13:05:14,955 - Epoch time: 25.49s, Average time: 25.64s
2024-03-15 13:05:14,955 - Loss decreased (4.69700e-04 --> 4.57020e-04).  Saving best model ...
2024-03-15 13:05:15,382 - 
2024-03-15 13:05:40,667 - Epoch: 258, Steps: 235 | Lr: 8.30724e-05 | Train Loss: 4.57386e-04
2024-03-15 13:05:40,667 - Epoch time: 25.28s, Average time: 25.64s
2024-03-15 13:05:40,977 - 
2024-03-15 13:06:06,587 - Epoch: 259, Steps: 235 | Lr: 8.27523e-05 | Train Loss: 4.71914e-04
2024-03-15 13:06:06,587 - Epoch time: 25.61s, Average time: 25.64s
2024-03-15 13:06:06,919 - 
2024-03-15 13:06:32,387 - Epoch: 260, Steps: 235 | Lr: 8.24298e-05 | Train Loss: 4.66170e-04
2024-03-15 13:06:32,388 - Epoch time: 25.47s, Average time: 25.64s
2024-03-15 13:06:32,722 - 
2024-03-15 13:06:58,334 - Epoch: 261, Steps: 235 | Lr: 8.21049e-05 | Train Loss: 4.51022e-04
2024-03-15 13:06:58,335 - Epoch time: 25.61s, Average time: 25.64s
2024-03-15 13:06:58,335 - Loss decreased (4.57020e-04 --> 4.51022e-04).  Saving best model ...
2024-03-15 13:06:58,763 - 
2024-03-15 13:07:24,258 - Epoch: 262, Steps: 235 | Lr: 8.17778e-05 | Train Loss: 4.56212e-04
2024-03-15 13:07:24,258 - Epoch time: 25.49s, Average time: 25.64s
2024-03-15 13:07:24,581 - 
2024-03-15 13:07:50,056 - Epoch: 263, Steps: 235 | Lr: 8.14483e-05 | Train Loss: 4.55870e-04
2024-03-15 13:07:50,057 - Epoch time: 25.48s, Average time: 25.64s
2024-03-15 13:07:50,372 - 
2024-03-15 13:08:15,648 - Epoch: 264, Steps: 235 | Lr: 8.11165e-05 | Train Loss: 4.31855e-04
2024-03-15 13:08:15,648 - Epoch time: 25.28s, Average time: 25.64s
2024-03-15 13:08:15,649 - Loss decreased (4.51022e-04 --> 4.31855e-04).  Saving best model ...
2024-03-15 13:08:16,069 - 
2024-03-15 13:08:41,779 - Epoch: 265, Steps: 235 | Lr: 8.07825e-05 | Train Loss: 4.30574e-04
2024-03-15 13:08:41,779 - Epoch time: 25.71s, Average time: 25.64s
2024-03-15 13:08:41,780 - Loss decreased (4.31855e-04 --> 4.30574e-04).  Saving best model ...
2024-03-15 13:08:42,201 - 
2024-03-15 13:09:07,899 - Epoch: 266, Steps: 235 | Lr: 8.04463e-05 | Train Loss: 4.32976e-04
2024-03-15 13:09:07,899 - Epoch time: 25.70s, Average time: 25.64s
2024-03-15 13:09:08,210 - 
2024-03-15 13:09:34,116 - Epoch: 267, Steps: 235 | Lr: 8.01079e-05 | Train Loss: 4.32000e-04
2024-03-15 13:09:34,116 - Epoch time: 25.91s, Average time: 25.64s
2024-03-15 13:09:34,448 - 
2024-03-15 13:10:00,444 - Epoch: 268, Steps: 235 | Lr: 7.97672e-05 | Train Loss: 4.16999e-04
2024-03-15 13:10:00,445 - Epoch time: 26.00s, Average time: 25.64s
2024-03-15 13:10:00,445 - Loss decreased (4.30574e-04 --> 4.16999e-04).  Saving best model ...
2024-03-15 13:10:00,862 - 
2024-03-15 13:10:26,683 - Epoch: 269, Steps: 235 | Lr: 7.94245e-05 | Train Loss: 4.62968e-04
2024-03-15 13:10:26,683 - Epoch time: 25.82s, Average time: 25.64s
2024-03-15 13:10:27,011 - 
2024-03-15 13:10:52,725 - Epoch: 270, Steps: 235 | Lr: 7.90796e-05 | Train Loss: 4.72646e-04
2024-03-15 13:10:52,725 - Epoch time: 25.71s, Average time: 25.64s
2024-03-15 13:10:53,042 - 
2024-03-15 13:11:18,619 - Epoch: 271, Steps: 235 | Lr: 7.87326e-05 | Train Loss: 4.06505e-04
2024-03-15 13:11:18,619 - Epoch time: 25.58s, Average time: 25.64s
2024-03-15 13:11:18,620 - Loss decreased (4.16999e-04 --> 4.06505e-04).  Saving best model ...
2024-03-15 13:11:19,044 - 
2024-03-15 13:11:44,563 - Epoch: 272, Steps: 235 | Lr: 7.83835e-05 | Train Loss: 4.27572e-04
2024-03-15 13:11:44,563 - Epoch time: 25.52s, Average time: 25.64s
2024-03-15 13:11:44,878 - 
2024-03-15 13:12:10,438 - Epoch: 273, Steps: 235 | Lr: 7.80324e-05 | Train Loss: 4.09861e-04
2024-03-15 13:12:10,438 - Epoch time: 25.56s, Average time: 25.64s
2024-03-15 13:12:10,751 - 
2024-03-15 13:12:36,452 - Epoch: 274, Steps: 235 | Lr: 7.76792e-05 | Train Loss: 3.96786e-04
2024-03-15 13:12:36,452 - Epoch time: 25.70s, Average time: 25.64s
2024-03-15 13:12:36,453 - Loss decreased (4.06505e-04 --> 3.96786e-04).  Saving best model ...
2024-03-15 13:12:37,017 - 
2024-03-15 13:13:02,851 - Epoch: 275, Steps: 235 | Lr: 7.73241e-05 | Train Loss: 4.01289e-04
2024-03-15 13:13:02,851 - Epoch time: 25.83s, Average time: 25.64s
2024-03-15 13:13:03,171 - 
2024-03-15 13:13:28,838 - Epoch: 276, Steps: 235 | Lr: 7.69669e-05 | Train Loss: 3.90878e-04
2024-03-15 13:13:28,838 - Epoch time: 25.67s, Average time: 25.64s
2024-03-15 13:13:28,839 - Loss decreased (3.96786e-04 --> 3.90878e-04).  Saving best model ...
2024-03-15 13:13:29,252 - 
2024-03-15 13:13:54,893 - Epoch: 277, Steps: 235 | Lr: 7.66078e-05 | Train Loss: 4.47255e-04
2024-03-15 13:13:54,893 - Epoch time: 25.64s, Average time: 25.64s
2024-03-15 13:13:55,209 - 
2024-03-15 13:14:20,862 - Epoch: 278, Steps: 235 | Lr: 7.62468e-05 | Train Loss: 4.02960e-04
2024-03-15 13:14:20,863 - Epoch time: 25.65s, Average time: 25.64s
2024-03-15 13:14:21,182 - 
2024-03-15 13:14:46,831 - Epoch: 279, Steps: 235 | Lr: 7.58839e-05 | Train Loss: 4.00772e-04
2024-03-15 13:14:46,831 - Epoch time: 25.65s, Average time: 25.64s
2024-03-15 13:14:47,154 - 
2024-03-15 13:15:12,960 - Epoch: 280, Steps: 235 | Lr: 7.55191e-05 | Train Loss: 3.94914e-04
2024-03-15 13:15:12,960 - Epoch time: 25.81s, Average time: 25.64s
2024-03-15 13:15:13,280 - 
2024-03-15 13:15:39,413 - Epoch: 281, Steps: 235 | Lr: 7.51525e-05 | Train Loss: 4.07976e-04
2024-03-15 13:15:39,413 - Epoch time: 26.13s, Average time: 25.64s
2024-03-15 13:15:39,732 - 
2024-03-15 13:16:05,229 - Epoch: 282, Steps: 235 | Lr: 7.47840e-05 | Train Loss: 4.06232e-04
2024-03-15 13:16:05,229 - Epoch time: 25.50s, Average time: 25.64s
2024-03-15 13:16:05,544 - 
2024-03-15 13:16:30,980 - Epoch: 283, Steps: 235 | Lr: 7.44138e-05 | Train Loss: 4.02185e-04
2024-03-15 13:16:30,980 - Epoch time: 25.44s, Average time: 25.64s
2024-03-15 13:16:31,309 - 
2024-03-15 13:16:56,717 - Epoch: 284, Steps: 235 | Lr: 7.40418e-05 | Train Loss: 3.87195e-04
2024-03-15 13:16:56,717 - Epoch time: 25.41s, Average time: 25.64s
2024-03-15 13:16:56,718 - Loss decreased (3.90878e-04 --> 3.87195e-04).  Saving best model ...
2024-03-15 13:16:57,125 - 
2024-03-15 13:17:22,541 - Epoch: 285, Steps: 235 | Lr: 7.36680e-05 | Train Loss: 3.78158e-04
2024-03-15 13:17:22,541 - Epoch time: 25.42s, Average time: 25.64s
2024-03-15 13:17:22,542 - Loss decreased (3.87195e-04 --> 3.78158e-04).  Saving best model ...
2024-03-15 13:17:22,979 - 
2024-03-15 13:17:48,516 - Epoch: 286, Steps: 235 | Lr: 7.32926e-05 | Train Loss: 3.75143e-04
2024-03-15 13:17:48,517 - Epoch time: 25.54s, Average time: 25.64s
2024-03-15 13:17:48,517 - Loss decreased (3.78158e-04 --> 3.75143e-04).  Saving best model ...
2024-03-15 13:17:48,953 - 
2024-03-15 13:18:14,503 - Epoch: 287, Steps: 235 | Lr: 7.29154e-05 | Train Loss: 3.84611e-04
2024-03-15 13:18:14,503 - Epoch time: 25.55s, Average time: 25.64s
2024-03-15 13:18:14,821 - 
2024-03-15 13:18:40,279 - Epoch: 288, Steps: 235 | Lr: 7.25366e-05 | Train Loss: 3.85636e-04
2024-03-15 13:18:40,279 - Epoch time: 25.46s, Average time: 25.64s
2024-03-15 13:18:40,607 - 
2024-03-15 13:19:06,329 - Epoch: 289, Steps: 235 | Lr: 7.21562e-05 | Train Loss: 3.87743e-04
2024-03-15 13:19:06,330 - Epoch time: 25.72s, Average time: 25.64s
2024-03-15 13:19:06,681 - 
2024-03-15 13:19:32,381 - Epoch: 290, Steps: 235 | Lr: 7.17742e-05 | Train Loss: 3.63824e-04
2024-03-15 13:19:32,382 - Epoch time: 25.70s, Average time: 25.64s
2024-03-15 13:19:32,382 - Loss decreased (3.75143e-04 --> 3.63824e-04).  Saving best model ...
2024-03-15 13:19:32,801 - 
2024-03-15 13:19:58,281 - Epoch: 291, Steps: 235 | Lr: 7.13905e-05 | Train Loss: 3.68373e-04
2024-03-15 13:19:58,282 - Epoch time: 25.48s, Average time: 25.64s
2024-03-15 13:19:58,612 - 
2024-03-15 13:20:23,969 - Epoch: 292, Steps: 235 | Lr: 7.10054e-05 | Train Loss: 3.59477e-04
2024-03-15 13:20:23,970 - Epoch time: 25.36s, Average time: 25.64s
2024-03-15 13:20:23,970 - Loss decreased (3.63824e-04 --> 3.59477e-04).  Saving best model ...
2024-03-15 13:20:24,396 - 
2024-03-15 13:20:49,761 - Epoch: 293, Steps: 235 | Lr: 7.06187e-05 | Train Loss: 3.71293e-04
2024-03-15 13:20:49,762 - Epoch time: 25.36s, Average time: 25.64s
2024-03-15 13:20:50,084 - 
2024-03-15 13:21:15,591 - Epoch: 294, Steps: 235 | Lr: 7.02305e-05 | Train Loss: 3.66043e-04
2024-03-15 13:21:15,592 - Epoch time: 25.51s, Average time: 25.64s
2024-03-15 13:21:15,908 - 
2024-03-15 13:21:41,354 - Epoch: 295, Steps: 235 | Lr: 6.98409e-05 | Train Loss: 3.67315e-04
2024-03-15 13:21:41,355 - Epoch time: 25.45s, Average time: 25.64s
2024-03-15 13:21:41,665 - 
2024-03-15 13:22:07,019 - Epoch: 296, Steps: 235 | Lr: 6.94498e-05 | Train Loss: 3.52300e-04
2024-03-15 13:22:07,019 - Epoch time: 25.35s, Average time: 25.64s
2024-03-15 13:22:07,019 - Loss decreased (3.59477e-04 --> 3.52300e-04).  Saving best model ...
2024-03-15 13:22:07,456 - 
2024-03-15 13:22:32,794 - Epoch: 297, Steps: 235 | Lr: 6.90574e-05 | Train Loss: 3.69009e-04
2024-03-15 13:22:32,794 - Epoch time: 25.34s, Average time: 25.63s
2024-03-15 13:22:33,126 - 
2024-03-15 13:22:58,819 - Epoch: 298, Steps: 235 | Lr: 6.86635e-05 | Train Loss: 3.95586e-04
2024-03-15 13:22:58,819 - Epoch time: 25.69s, Average time: 25.63s
2024-03-15 13:22:59,134 - 
2024-03-15 13:23:24,388 - Epoch: 299, Steps: 235 | Lr: 6.82683e-05 | Train Loss: 3.56512e-04
2024-03-15 13:23:24,389 - Epoch time: 25.25s, Average time: 25.63s
2024-03-15 13:23:24,709 - 
2024-03-15 13:23:50,269 - Epoch: 300, Steps: 235 | Lr: 6.78718e-05 | Train Loss: 3.48880e-04
2024-03-15 13:23:50,269 - Epoch time: 25.56s, Average time: 25.63s
2024-03-15 13:23:50,269 - Loss decreased (3.52300e-04 --> 3.48880e-04).  Saving best model ...
2024-03-15 13:23:50,375 - Save the loss_recoder of 300 epochs
2024-03-15 13:23:50,396 - Simulation picture saved in /home/bingxing2/home/scx6mfb/mhy/SAU/CFD-CNN/Model/SAU_1001_10to1-0.3/pic/loss_record.png
2024-03-15 13:23:51,532 - 
2024-03-15 13:24:16,950 - Epoch: 301, Steps: 235 | Lr: 6.74740e-05 | Train Loss: 3.44656e-04
2024-03-15 13:24:16,950 - Epoch time: 25.42s, Average time: 25.63s
2024-03-15 13:24:16,950 - Loss decreased (3.48880e-04 --> 3.44656e-04).  Saving best model ...
2024-03-15 13:24:17,365 - 
2024-03-15 13:24:43,150 - Epoch: 302, Steps: 235 | Lr: 6.70749e-05 | Train Loss: 3.63923e-04
2024-03-15 13:24:43,150 - Epoch time: 25.78s, Average time: 25.63s
2024-03-15 13:24:43,476 - 
2024-03-15 13:25:09,082 - Epoch: 303, Steps: 235 | Lr: 6.66746e-05 | Train Loss: 3.58283e-04
2024-03-15 13:25:09,082 - Epoch time: 25.60s, Average time: 25.63s
2024-03-15 13:25:09,400 - 
2024-03-15 13:25:35,122 - Epoch: 304, Steps: 235 | Lr: 6.62730e-05 | Train Loss: 3.66202e-04
2024-03-15 13:25:35,122 - Epoch time: 25.72s, Average time: 25.63s
2024-03-15 13:25:35,467 - 
2024-03-15 13:26:01,099 - Epoch: 305, Steps: 235 | Lr: 6.58703e-05 | Train Loss: 3.39730e-04
2024-03-15 13:26:01,099 - Epoch time: 25.63s, Average time: 25.63s
2024-03-15 13:26:01,100 - Loss decreased (3.44656e-04 --> 3.39730e-04).  Saving best model ...
2024-03-15 13:26:01,515 - 
2024-03-15 13:26:27,208 - Epoch: 306, Steps: 235 | Lr: 6.54665e-05 | Train Loss: 3.46634e-04
2024-03-15 13:26:27,208 - Epoch time: 25.69s, Average time: 25.63s
2024-03-15 13:26:27,529 - 
2024-03-15 13:26:53,275 - Epoch: 307, Steps: 235 | Lr: 6.50615e-05 | Train Loss: 3.50210e-04
2024-03-15 13:26:53,275 - Epoch time: 25.75s, Average time: 25.63s
2024-03-15 13:26:53,596 - 
2024-03-15 13:27:19,051 - Epoch: 308, Steps: 235 | Lr: 6.46554e-05 | Train Loss: 3.42116e-04
2024-03-15 13:27:19,051 - Epoch time: 25.45s, Average time: 25.63s
2024-03-15 13:27:19,369 - 
2024-03-15 13:27:44,941 - Epoch: 309, Steps: 235 | Lr: 6.42483e-05 | Train Loss: 3.41442e-04
2024-03-15 13:27:44,942 - Epoch time: 25.57s, Average time: 25.63s
2024-03-15 13:27:45,291 - 
2024-03-15 13:28:10,831 - Epoch: 310, Steps: 235 | Lr: 6.38402e-05 | Train Loss: 3.37942e-04
2024-03-15 13:28:10,831 - Epoch time: 25.54s, Average time: 25.63s
2024-03-15 13:28:10,831 - Loss decreased (3.39730e-04 --> 3.37942e-04).  Saving best model ...
2024-03-15 13:28:11,246 - 
2024-03-15 13:28:36,825 - Epoch: 311, Steps: 235 | Lr: 6.34310e-05 | Train Loss: 3.33371e-04
2024-03-15 13:28:36,825 - Epoch time: 25.58s, Average time: 25.63s
2024-03-15 13:28:36,825 - Loss decreased (3.37942e-04 --> 3.33371e-04).  Saving best model ...
2024-03-15 13:28:37,363 - 
2024-03-15 13:29:03,006 - Epoch: 312, Steps: 235 | Lr: 6.30209e-05 | Train Loss: 3.36303e-04
2024-03-15 13:29:03,006 - Epoch time: 25.64s, Average time: 25.63s
2024-03-15 13:29:03,324 - 
2024-03-15 13:29:29,020 - Epoch: 313, Steps: 235 | Lr: 6.26098e-05 | Train Loss: 3.23336e-04
2024-03-15 13:29:29,020 - Epoch time: 25.70s, Average time: 25.63s
2024-03-15 13:29:29,020 - Loss decreased (3.33371e-04 --> 3.23336e-04).  Saving best model ...
2024-03-15 13:29:29,434 - 
2024-03-15 13:29:55,041 - Epoch: 314, Steps: 235 | Lr: 6.21979e-05 | Train Loss: 3.20612e-04
2024-03-15 13:29:55,042 - Epoch time: 25.61s, Average time: 25.63s
2024-03-15 13:29:55,042 - Loss decreased (3.23336e-04 --> 3.20612e-04).  Saving best model ...
2024-03-15 13:29:55,457 - 
2024-03-15 13:30:21,301 - Epoch: 315, Steps: 235 | Lr: 6.17850e-05 | Train Loss: 3.28889e-04
2024-03-15 13:30:21,301 - Epoch time: 25.84s, Average time: 25.63s
2024-03-15 13:30:21,616 - 
2024-03-15 13:30:47,400 - Epoch: 316, Steps: 235 | Lr: 6.13713e-05 | Train Loss: 3.32562e-04
2024-03-15 13:30:47,401 - Epoch time: 25.78s, Average time: 25.63s
2024-03-15 13:30:47,725 - 
2024-03-15 13:31:13,278 - Epoch: 317, Steps: 235 | Lr: 6.09568e-05 | Train Loss: 3.34642e-04
2024-03-15 13:31:13,278 - Epoch time: 25.55s, Average time: 25.63s
2024-03-15 13:31:13,593 - 
2024-03-15 13:31:39,593 - Epoch: 318, Steps: 235 | Lr: 6.05414e-05 | Train Loss: 3.21774e-04
2024-03-15 13:31:39,593 - Epoch time: 26.00s, Average time: 25.63s
2024-03-15 13:31:39,919 - 
2024-03-15 13:32:05,609 - Epoch: 319, Steps: 235 | Lr: 6.01253e-05 | Train Loss: 3.09881e-04
2024-03-15 13:32:05,609 - Epoch time: 25.69s, Average time: 25.63s
2024-03-15 13:32:05,610 - Loss decreased (3.20612e-04 --> 3.09881e-04).  Saving best model ...
2024-03-15 13:32:06,046 - 
2024-03-15 13:32:31,724 - Epoch: 320, Steps: 235 | Lr: 5.97085e-05 | Train Loss: 3.22552e-04
2024-03-15 13:32:31,725 - Epoch time: 25.68s, Average time: 25.63s
2024-03-15 13:32:32,042 - 
2024-03-15 13:32:57,729 - Epoch: 321, Steps: 235 | Lr: 5.92910e-05 | Train Loss: 3.09343e-04
2024-03-15 13:32:57,729 - Epoch time: 25.69s, Average time: 25.64s
2024-03-15 13:32:57,729 - Loss decreased (3.09881e-04 --> 3.09343e-04).  Saving best model ...
2024-03-15 13:32:58,143 - 
2024-03-15 13:33:23,747 - Epoch: 322, Steps: 235 | Lr: 5.88728e-05 | Train Loss: 3.18792e-04
2024-03-15 13:33:23,747 - Epoch time: 25.60s, Average time: 25.63s
2024-03-15 13:33:24,060 - 
2024-03-15 13:33:49,626 - Epoch: 323, Steps: 235 | Lr: 5.84540e-05 | Train Loss: 3.18618e-04
2024-03-15 13:33:49,626 - Epoch time: 25.57s, Average time: 25.63s
2024-03-15 13:33:49,928 - 
2024-03-15 13:34:15,949 - Epoch: 324, Steps: 235 | Lr: 5.80345e-05 | Train Loss: 3.30475e-04
2024-03-15 13:34:15,949 - Epoch time: 26.02s, Average time: 25.64s
2024-03-15 13:34:16,258 - 
2024-03-15 13:34:41,861 - Epoch: 325, Steps: 235 | Lr: 5.76145e-05 | Train Loss: 3.03034e-04
2024-03-15 13:34:41,861 - Epoch time: 25.60s, Average time: 25.64s
2024-03-15 13:34:41,862 - Loss decreased (3.09343e-04 --> 3.03034e-04).  Saving best model ...
2024-03-15 13:34:42,265 - 
2024-03-15 13:35:08,065 - Epoch: 326, Steps: 235 | Lr: 5.71939e-05 | Train Loss: 3.14866e-04
2024-03-15 13:35:08,065 - Epoch time: 25.80s, Average time: 25.64s
2024-03-15 13:35:08,390 - 
2024-03-15 13:35:34,133 - Epoch: 327, Steps: 235 | Lr: 5.67728e-05 | Train Loss: 3.10965e-04
2024-03-15 13:35:34,133 - Epoch time: 25.74s, Average time: 25.64s
2024-03-15 13:35:34,456 - 
2024-03-15 13:36:00,512 - Epoch: 328, Steps: 235 | Lr: 5.63512e-05 | Train Loss: 2.96526e-04
2024-03-15 13:36:00,512 - Epoch time: 26.05s, Average time: 25.64s
2024-03-15 13:36:00,512 - Loss decreased (3.03034e-04 --> 2.96526e-04).  Saving best model ...
2024-03-15 13:36:00,936 - 
2024-03-15 13:36:26,576 - Epoch: 329, Steps: 235 | Lr: 5.59292e-05 | Train Loss: 3.00748e-04
2024-03-15 13:36:26,576 - Epoch time: 25.64s, Average time: 25.64s
2024-03-15 13:36:26,903 - 
2024-03-15 13:36:52,362 - Epoch: 330, Steps: 235 | Lr: 5.55067e-05 | Train Loss: 2.94473e-04
2024-03-15 13:36:52,362 - Epoch time: 25.46s, Average time: 25.64s
2024-03-15 13:36:52,362 - Loss decreased (2.96526e-04 --> 2.94473e-04).  Saving best model ...
2024-03-15 13:36:52,780 - 
2024-03-15 13:37:18,217 - Epoch: 331, Steps: 235 | Lr: 5.50838e-05 | Train Loss: 2.92593e-04
2024-03-15 13:37:18,217 - Epoch time: 25.44s, Average time: 25.64s
2024-03-15 13:37:18,217 - Loss decreased (2.94473e-04 --> 2.92593e-04).  Saving best model ...
2024-03-15 13:37:18,644 - 
2024-03-15 13:37:44,128 - Epoch: 332, Steps: 235 | Lr: 5.46606e-05 | Train Loss: 2.94411e-04
2024-03-15 13:37:44,128 - Epoch time: 25.48s, Average time: 25.64s
2024-03-15 13:37:44,444 - 
2024-03-15 13:38:10,062 - Epoch: 333, Steps: 235 | Lr: 5.42370e-05 | Train Loss: 3.02969e-04
2024-03-15 13:38:10,063 - Epoch time: 25.62s, Average time: 25.64s
2024-03-15 13:38:10,408 - 
2024-03-15 13:38:36,260 - Epoch: 334, Steps: 235 | Lr: 5.38131e-05 | Train Loss: 2.98338e-04
2024-03-15 13:38:36,260 - Epoch time: 25.85s, Average time: 25.64s
2024-03-15 13:38:36,568 - 
2024-03-15 13:39:02,004 - Epoch: 335, Steps: 235 | Lr: 5.33889e-05 | Train Loss: 2.94707e-04
2024-03-15 13:39:02,005 - Epoch time: 25.44s, Average time: 25.64s
2024-03-15 13:39:02,336 - 
2024-03-15 13:39:28,026 - Epoch: 336, Steps: 235 | Lr: 5.29645e-05 | Train Loss: 2.94749e-04
2024-03-15 13:39:28,026 - Epoch time: 25.69s, Average time: 25.64s
2024-03-15 13:39:28,332 - 
2024-03-15 13:39:53,611 - Epoch: 337, Steps: 235 | Lr: 5.25399e-05 | Train Loss: 2.87006e-04
2024-03-15 13:39:53,611 - Epoch time: 25.28s, Average time: 25.64s
2024-03-15 13:39:53,611 - Loss decreased (2.92593e-04 --> 2.87006e-04).  Saving best model ...
2024-03-15 13:39:54,023 - 
2024-03-15 13:40:19,437 - Epoch: 338, Steps: 235 | Lr: 5.21151e-05 | Train Loss: 2.92232e-04
2024-03-15 13:40:19,437 - Epoch time: 25.41s, Average time: 25.63s
2024-03-15 13:40:19,773 - 
2024-03-15 13:40:45,400 - Epoch: 339, Steps: 235 | Lr: 5.16901e-05 | Train Loss: 2.78384e-04
2024-03-15 13:40:45,400 - Epoch time: 25.63s, Average time: 25.63s
2024-03-15 13:40:45,401 - Loss decreased (2.87006e-04 --> 2.78384e-04).  Saving best model ...
2024-03-15 13:40:45,843 - 
2024-03-15 13:41:11,800 - Epoch: 340, Steps: 235 | Lr: 5.12650e-05 | Train Loss: 2.80459e-04
2024-03-15 13:41:11,800 - Epoch time: 25.96s, Average time: 25.64s
2024-03-15 13:41:12,121 - 
2024-03-15 13:41:37,859 - Epoch: 341, Steps: 235 | Lr: 5.08398e-05 | Train Loss: 2.87125e-04
2024-03-15 13:41:37,860 - Epoch time: 25.74s, Average time: 25.64s
2024-03-15 13:41:38,182 - 
2024-03-15 13:42:04,012 - Epoch: 342, Steps: 235 | Lr: 5.04146e-05 | Train Loss: 2.90257e-04
2024-03-15 13:42:04,013 - Epoch time: 25.83s, Average time: 25.64s
2024-03-15 13:42:04,360 - 
2024-03-15 13:42:30,027 - Epoch: 343, Steps: 235 | Lr: 4.99893e-05 | Train Loss: 2.81491e-04
2024-03-15 13:42:30,027 - Epoch time: 25.67s, Average time: 25.64s
2024-03-15 13:42:30,342 - 
2024-03-15 13:42:55,774 - Epoch: 344, Steps: 235 | Lr: 4.95641e-05 | Train Loss: 2.78330e-04
2024-03-15 13:42:55,774 - Epoch time: 25.43s, Average time: 25.64s
2024-03-15 13:42:55,775 - Loss decreased (2.78384e-04 --> 2.78330e-04).  Saving best model ...
2024-03-15 13:42:56,188 - 
2024-03-15 13:43:21,646 - Epoch: 345, Steps: 235 | Lr: 4.91388e-05 | Train Loss: 2.79766e-04
2024-03-15 13:43:21,646 - Epoch time: 25.46s, Average time: 25.64s
2024-03-15 13:43:22,069 - 
2024-03-15 13:43:47,666 - Epoch: 346, Steps: 235 | Lr: 4.87137e-05 | Train Loss: 2.81100e-04
2024-03-15 13:43:47,667 - Epoch time: 25.60s, Average time: 25.64s
2024-03-15 13:43:47,987 - 
2024-03-15 13:44:13,670 - Epoch: 347, Steps: 235 | Lr: 4.82886e-05 | Train Loss: 2.80324e-04
2024-03-15 13:44:13,670 - Epoch time: 25.68s, Average time: 25.64s
2024-03-15 13:44:13,982 - 
2024-03-15 13:44:39,540 - Epoch: 348, Steps: 235 | Lr: 4.78636e-05 | Train Loss: 2.72084e-04
2024-03-15 13:44:39,541 - Epoch time: 25.56s, Average time: 25.64s
2024-03-15 13:44:39,541 - Loss decreased (2.78330e-04 --> 2.72084e-04).  Saving best model ...
2024-03-15 13:44:39,951 - 
2024-03-15 13:45:05,539 - Epoch: 349, Steps: 235 | Lr: 4.74388e-05 | Train Loss: 2.70544e-04
2024-03-15 13:45:05,540 - Epoch time: 25.59s, Average time: 25.64s
2024-03-15 13:45:05,540 - Loss decreased (2.72084e-04 --> 2.70544e-04).  Saving best model ...
2024-03-15 13:45:05,969 - 
2024-03-15 13:45:31,531 - Epoch: 350, Steps: 235 | Lr: 4.70142e-05 | Train Loss: 2.77800e-04
2024-03-15 13:45:31,531 - Epoch time: 25.56s, Average time: 25.63s
2024-03-15 13:45:31,536 - Save the loss_recoder of 350 epochs
2024-03-15 13:45:31,557 - Simulation picture saved in /home/bingxing2/home/scx6mfb/mhy/SAU/CFD-CNN/Model/SAU_1001_10to1-0.3/pic/loss_record.png
2024-03-15 13:45:32,585 - 
2024-03-15 13:45:57,918 - Epoch: 351, Steps: 235 | Lr: 4.65898e-05 | Train Loss: 2.76301e-04
2024-03-15 13:45:57,918 - Epoch time: 25.33s, Average time: 25.63s
2024-03-15 13:45:58,221 - 
2024-03-15 13:46:23,556 - Epoch: 352, Steps: 235 | Lr: 4.61657e-05 | Train Loss: 2.76454e-04
2024-03-15 13:46:23,556 - Epoch time: 25.33s, Average time: 25.63s
2024-03-15 13:46:23,883 - 
2024-03-15 13:46:49,195 - Epoch: 353, Steps: 235 | Lr: 4.57418e-05 | Train Loss: 2.69715e-04
2024-03-15 13:46:49,196 - Epoch time: 25.31s, Average time: 25.63s
2024-03-15 13:46:49,196 - Loss decreased (2.70544e-04 --> 2.69715e-04).  Saving best model ...
2024-03-15 13:46:49,610 - 
2024-03-15 13:47:14,728 - Epoch: 354, Steps: 235 | Lr: 4.53182e-05 | Train Loss: 2.70280e-04
2024-03-15 13:47:14,728 - Epoch time: 25.12s, Average time: 25.63s
2024-03-15 13:47:15,042 - 
2024-03-15 13:47:40,437 - Epoch: 355, Steps: 235 | Lr: 4.48950e-05 | Train Loss: 2.70330e-04
2024-03-15 13:47:40,437 - Epoch time: 25.39s, Average time: 25.63s
2024-03-15 13:47:40,746 - 
2024-03-15 13:48:06,175 - Epoch: 356, Steps: 235 | Lr: 4.44721e-05 | Train Loss: 2.58296e-04
2024-03-15 13:48:06,175 - Epoch time: 25.43s, Average time: 25.63s
2024-03-15 13:48:06,175 - Loss decreased (2.69715e-04 --> 2.58296e-04).  Saving best model ...
2024-03-15 13:48:06,611 - 
2024-03-15 13:48:32,168 - Epoch: 357, Steps: 235 | Lr: 4.40497e-05 | Train Loss: 2.62648e-04
2024-03-15 13:48:32,168 - Epoch time: 25.56s, Average time: 25.63s
2024-03-15 13:48:32,504 - 
2024-03-15 13:48:57,849 - Epoch: 358, Steps: 235 | Lr: 4.36276e-05 | Train Loss: 2.64469e-04
2024-03-15 13:48:57,850 - Epoch time: 25.35s, Average time: 25.63s
2024-03-15 13:48:58,279 - 
2024-03-15 13:49:23,771 - Epoch: 359, Steps: 235 | Lr: 4.32061e-05 | Train Loss: 2.64370e-04
2024-03-15 13:49:23,771 - Epoch time: 25.49s, Average time: 25.63s
2024-03-15 13:49:24,097 - 
2024-03-15 13:49:49,443 - Epoch: 360, Steps: 235 | Lr: 4.27850e-05 | Train Loss: 2.60691e-04
2024-03-15 13:49:49,444 - Epoch time: 25.35s, Average time: 25.63s
2024-03-15 13:49:49,746 - 
2024-03-15 13:50:15,123 - Epoch: 361, Steps: 235 | Lr: 4.23645e-05 | Train Loss: 2.65526e-04
2024-03-15 13:50:15,124 - Epoch time: 25.38s, Average time: 25.63s
2024-03-15 13:50:15,437 - 
2024-03-15 13:50:40,952 - Epoch: 362, Steps: 235 | Lr: 4.19444e-05 | Train Loss: 2.59548e-04
2024-03-15 13:50:40,953 - Epoch time: 25.52s, Average time: 25.63s
2024-03-15 13:50:41,277 - 
2024-03-15 13:51:07,080 - Epoch: 363, Steps: 235 | Lr: 4.15250e-05 | Train Loss: 2.51588e-04
2024-03-15 13:51:07,080 - Epoch time: 25.80s, Average time: 25.63s
2024-03-15 13:51:07,081 - Loss decreased (2.58296e-04 --> 2.51588e-04).  Saving best model ...
2024-03-15 13:51:07,497 - 
2024-03-15 13:51:33,065 - Epoch: 364, Steps: 235 | Lr: 4.11062e-05 | Train Loss: 2.55661e-04
2024-03-15 13:51:33,065 - Epoch time: 25.57s, Average time: 25.63s
2024-03-15 13:51:33,377 - 
2024-03-15 13:51:58,835 - Epoch: 365, Steps: 235 | Lr: 4.06881e-05 | Train Loss: 2.58878e-04
2024-03-15 13:51:58,835 - Epoch time: 25.46s, Average time: 25.63s
2024-03-15 13:51:59,162 - 
2024-03-15 13:52:24,648 - Epoch: 366, Steps: 235 | Lr: 4.02706e-05 | Train Loss: 2.52895e-04
2024-03-15 13:52:24,649 - Epoch time: 25.49s, Average time: 25.63s
2024-03-15 13:52:24,955 - 
2024-03-15 13:52:50,376 - Epoch: 367, Steps: 235 | Lr: 3.98538e-05 | Train Loss: 2.50282e-04
2024-03-15 13:52:50,376 - Epoch time: 25.42s, Average time: 25.63s
2024-03-15 13:52:50,377 - Loss decreased (2.51588e-04 --> 2.50282e-04).  Saving best model ...
2024-03-15 13:52:50,791 - 
2024-03-15 13:53:16,089 - Epoch: 368, Steps: 235 | Lr: 3.94377e-05 | Train Loss: 2.54670e-04
2024-03-15 13:53:16,089 - Epoch time: 25.30s, Average time: 25.62s
2024-03-15 13:53:16,401 - 
2024-03-15 13:53:41,793 - Epoch: 369, Steps: 235 | Lr: 3.90224e-05 | Train Loss: 2.47072e-04
2024-03-15 13:53:41,793 - Epoch time: 25.39s, Average time: 25.62s
2024-03-15 13:53:41,793 - Loss decreased (2.50282e-04 --> 2.47072e-04).  Saving best model ...
2024-03-15 13:53:42,214 - 
2024-03-15 13:54:07,725 - Epoch: 370, Steps: 235 | Lr: 3.86080e-05 | Train Loss: 2.52566e-04
2024-03-15 13:54:07,725 - Epoch time: 25.51s, Average time: 25.62s
2024-03-15 13:54:08,053 - 
2024-03-15 13:54:33,552 - Epoch: 371, Steps: 235 | Lr: 3.81943e-05 | Train Loss: 2.51785e-04
2024-03-15 13:54:33,552 - Epoch time: 25.50s, Average time: 25.62s
2024-03-15 13:54:33,872 - 
2024-03-15 13:54:59,707 - Epoch: 372, Steps: 235 | Lr: 3.77815e-05 | Train Loss: 2.53621e-04
2024-03-15 13:54:59,707 - Epoch time: 25.83s, Average time: 25.62s
2024-03-15 13:55:00,027 - 
2024-03-15 13:55:25,813 - Epoch: 373, Steps: 235 | Lr: 3.73695e-05 | Train Loss: 2.44076e-04
2024-03-15 13:55:25,813 - Epoch time: 25.78s, Average time: 25.62s
2024-03-15 13:55:25,813 - Loss decreased (2.47072e-04 --> 2.44076e-04).  Saving best model ...
2024-03-15 13:55:26,352 - 
2024-03-15 13:55:51,925 - Epoch: 374, Steps: 235 | Lr: 3.69585e-05 | Train Loss: 2.49370e-04
2024-03-15 13:55:51,925 - Epoch time: 25.57s, Average time: 25.62s
2024-03-15 13:55:52,236 - 
2024-03-15 13:56:17,566 - Epoch: 375, Steps: 235 | Lr: 3.65485e-05 | Train Loss: 2.52303e-04
2024-03-15 13:56:17,566 - Epoch time: 25.33s, Average time: 25.62s
2024-03-15 13:56:17,874 - 
2024-03-15 13:56:43,276 - Epoch: 376, Steps: 235 | Lr: 3.61394e-05 | Train Loss: 2.44680e-04
2024-03-15 13:56:43,277 - Epoch time: 25.40s, Average time: 25.62s
2024-03-15 13:56:43,598 - 
2024-03-15 13:57:09,070 - Epoch: 377, Steps: 235 | Lr: 3.57313e-05 | Train Loss: 2.38559e-04
2024-03-15 13:57:09,070 - Epoch time: 25.47s, Average time: 25.62s
2024-03-15 13:57:09,070 - Loss decreased (2.44076e-04 --> 2.38559e-04).  Saving best model ...
2024-03-15 13:57:09,509 - 
2024-03-15 13:57:34,977 - Epoch: 378, Steps: 235 | Lr: 3.53242e-05 | Train Loss: 2.40321e-04
2024-03-15 13:57:34,977 - Epoch time: 25.47s, Average time: 25.62s
2024-03-15 13:57:35,305 - 
2024-03-15 13:58:00,705 - Epoch: 379, Steps: 235 | Lr: 3.49182e-05 | Train Loss: 2.49586e-04
2024-03-15 13:58:00,706 - Epoch time: 25.40s, Average time: 25.62s
2024-03-15 13:58:01,026 - 
2024-03-15 13:58:26,451 - Epoch: 380, Steps: 235 | Lr: 3.45133e-05 | Train Loss: 2.37432e-04
2024-03-15 13:58:26,451 - Epoch time: 25.42s, Average time: 25.62s
2024-03-15 13:58:26,452 - Loss decreased (2.38559e-04 --> 2.37432e-04).  Saving best model ...
2024-03-15 13:58:26,872 - 
2024-03-15 13:58:52,419 - Epoch: 381, Steps: 235 | Lr: 3.41095e-05 | Train Loss: 2.40980e-04
2024-03-15 13:58:52,419 - Epoch time: 25.55s, Average time: 25.62s
2024-03-15 13:58:52,733 - 
2024-03-15 13:59:18,202 - Epoch: 382, Steps: 235 | Lr: 3.37068e-05 | Train Loss: 2.34193e-04
2024-03-15 13:59:18,202 - Epoch time: 25.47s, Average time: 25.62s
2024-03-15 13:59:18,203 - Loss decreased (2.37432e-04 --> 2.34193e-04).  Saving best model ...
2024-03-15 13:59:18,622 - 
2024-03-15 13:59:44,021 - Epoch: 383, Steps: 235 | Lr: 3.33054e-05 | Train Loss: 2.36555e-04
2024-03-15 13:59:44,021 - Epoch time: 25.40s, Average time: 25.62s
2024-03-15 13:59:44,339 - 
2024-03-15 14:00:09,971 - Epoch: 384, Steps: 235 | Lr: 3.29051e-05 | Train Loss: 2.36937e-04
2024-03-15 14:00:09,971 - Epoch time: 25.63s, Average time: 25.62s
2024-03-15 14:00:10,287 - 
2024-03-15 14:00:35,612 - Epoch: 385, Steps: 235 | Lr: 3.25061e-05 | Train Loss: 2.31410e-04
2024-03-15 14:00:35,612 - Epoch time: 25.32s, Average time: 25.62s
2024-03-15 14:00:35,612 - Loss decreased (2.34193e-04 --> 2.31410e-04).  Saving best model ...
2024-03-15 14:00:36,015 - 
2024-03-15 14:01:01,346 - Epoch: 386, Steps: 235 | Lr: 3.21083e-05 | Train Loss: 2.38582e-04
2024-03-15 14:01:01,347 - Epoch time: 25.33s, Average time: 25.62s
2024-03-15 14:01:01,685 - 
2024-03-15 14:01:27,112 - Epoch: 387, Steps: 235 | Lr: 3.17119e-05 | Train Loss: 2.37176e-04
2024-03-15 14:01:27,113 - Epoch time: 25.43s, Average time: 25.62s
2024-03-15 14:01:27,420 - 
2024-03-15 14:01:52,913 - Epoch: 388, Steps: 235 | Lr: 3.13167e-05 | Train Loss: 2.29413e-04
2024-03-15 14:01:52,913 - Epoch time: 25.49s, Average time: 25.62s
2024-03-15 14:01:52,913 - Loss decreased (2.31410e-04 --> 2.29413e-04).  Saving best model ...
2024-03-15 14:01:53,322 - 
2024-03-15 14:02:18,891 - Epoch: 389, Steps: 235 | Lr: 3.09230e-05 | Train Loss: 2.34088e-04
2024-03-15 14:02:18,891 - Epoch time: 25.57s, Average time: 25.62s
2024-03-15 14:02:19,330 - 
2024-03-15 14:02:44,685 - Epoch: 390, Steps: 235 | Lr: 3.05306e-05 | Train Loss: 2.33618e-04
2024-03-15 14:02:44,685 - Epoch time: 25.35s, Average time: 25.62s
2024-03-15 14:02:44,988 - 
2024-03-15 14:03:10,401 - Epoch: 391, Steps: 235 | Lr: 3.01396e-05 | Train Loss: 2.33482e-04
2024-03-15 14:03:10,401 - Epoch time: 25.41s, Average time: 25.62s
2024-03-15 14:03:10,729 - 
2024-03-15 14:03:36,120 - Epoch: 392, Steps: 235 | Lr: 2.97500e-05 | Train Loss: 2.39036e-04
2024-03-15 14:03:36,120 - Epoch time: 25.39s, Average time: 25.62s
2024-03-15 14:03:36,436 - 
2024-03-15 14:04:01,906 - Epoch: 393, Steps: 235 | Lr: 2.93619e-05 | Train Loss: 2.37056e-04
2024-03-15 14:04:01,906 - Epoch time: 25.47s, Average time: 25.62s
2024-03-15 14:04:02,239 - 
2024-03-15 14:04:27,818 - Epoch: 394, Steps: 235 | Lr: 2.89753e-05 | Train Loss: 2.22692e-04
2024-03-15 14:04:27,819 - Epoch time: 25.58s, Average time: 25.61s
2024-03-15 14:04:27,819 - Loss decreased (2.29413e-04 --> 2.22692e-04).  Saving best model ...
2024-03-15 14:04:28,231 - 
2024-03-15 14:04:53,826 - Epoch: 395, Steps: 235 | Lr: 2.85902e-05 | Train Loss: 2.20359e-04
2024-03-15 14:04:53,826 - Epoch time: 25.59s, Average time: 25.61s
2024-03-15 14:04:53,826 - Loss decreased (2.22692e-04 --> 2.20359e-04).  Saving best model ...
2024-03-15 14:04:54,268 - 
2024-03-15 14:05:19,684 - Epoch: 396, Steps: 235 | Lr: 2.82067e-05 | Train Loss: 2.33896e-04
2024-03-15 14:05:19,685 - Epoch time: 25.42s, Average time: 25.61s
2024-03-15 14:05:20,006 - 
2024-03-15 14:05:45,552 - Epoch: 397, Steps: 235 | Lr: 2.78247e-05 | Train Loss: 2.23277e-04
2024-03-15 14:05:45,552 - Epoch time: 25.55s, Average time: 25.61s
2024-03-15 14:05:45,878 - 
2024-03-15 14:06:11,422 - Epoch: 398, Steps: 235 | Lr: 2.74444e-05 | Train Loss: 2.32120e-04
2024-03-15 14:06:11,423 - Epoch time: 25.54s, Average time: 25.61s
2024-03-15 14:06:11,739 - 
2024-03-15 14:06:37,386 - Epoch: 399, Steps: 235 | Lr: 2.70657e-05 | Train Loss: 2.29960e-04
2024-03-15 14:06:37,387 - Epoch time: 25.65s, Average time: 25.61s
2024-03-15 14:06:37,692 - 
2024-03-15 14:07:03,039 - Epoch: 400, Steps: 235 | Lr: 2.66886e-05 | Train Loss: 2.24545e-04
2024-03-15 14:07:03,039 - Epoch time: 25.35s, Average time: 25.61s
2024-03-15 14:07:03,044 - Save the loss_recoder of 400 epochs
2024-03-15 14:07:03,065 - Simulation picture saved in /home/bingxing2/home/scx6mfb/mhy/SAU/CFD-CNN/Model/SAU_1001_10to1-0.3/pic/loss_record.png
2024-03-15 14:07:04,209 - 
2024-03-15 14:07:30,046 - Epoch: 401, Steps: 235 | Lr: 2.63132e-05 | Train Loss: 2.19216e-04
2024-03-15 14:07:30,046 - Epoch time: 25.84s, Average time: 25.61s
2024-03-15 14:07:30,047 - Loss decreased (2.20359e-04 --> 2.19216e-04).  Saving best model ...
2024-03-15 14:07:30,467 - 
2024-03-15 14:07:56,158 - Epoch: 402, Steps: 235 | Lr: 2.59396e-05 | Train Loss: 2.24829e-04
2024-03-15 14:07:56,158 - Epoch time: 25.69s, Average time: 25.61s
2024-03-15 14:07:56,488 - 
2024-03-15 14:08:22,055 - Epoch: 403, Steps: 235 | Lr: 2.55677e-05 | Train Loss: 2.24035e-04
2024-03-15 14:08:22,055 - Epoch time: 25.57s, Average time: 25.61s
2024-03-15 14:08:22,372 - 
2024-03-15 14:08:47,889 - Epoch: 404, Steps: 235 | Lr: 2.51975e-05 | Train Loss: 2.22411e-04
2024-03-15 14:08:47,889 - Epoch time: 25.52s, Average time: 25.61s
2024-03-15 14:08:48,197 - 
2024-03-15 14:09:13,884 - Epoch: 405, Steps: 235 | Lr: 2.48291e-05 | Train Loss: 2.21485e-04
2024-03-15 14:09:13,884 - Epoch time: 25.69s, Average time: 25.61s
2024-03-15 14:09:14,199 - 
2024-03-15 14:09:39,829 - Epoch: 406, Steps: 235 | Lr: 2.44626e-05 | Train Loss: 2.17017e-04
2024-03-15 14:09:39,829 - Epoch time: 25.63s, Average time: 25.61s
2024-03-15 14:09:39,829 - Loss decreased (2.19216e-04 --> 2.17017e-04).  Saving best model ...
2024-03-15 14:09:40,262 - 
2024-03-15 14:10:05,884 - Epoch: 407, Steps: 235 | Lr: 2.40979e-05 | Train Loss: 2.14769e-04
2024-03-15 14:10:05,884 - Epoch time: 25.62s, Average time: 25.61s
2024-03-15 14:10:05,885 - Loss decreased (2.17017e-04 --> 2.14769e-04).  Saving best model ...
2024-03-15 14:10:06,314 - 
2024-03-15 14:10:32,062 - Epoch: 408, Steps: 235 | Lr: 2.37351e-05 | Train Loss: 2.18613e-04
2024-03-15 14:10:32,062 - Epoch time: 25.75s, Average time: 25.61s
2024-03-15 14:10:32,373 - 
2024-03-15 14:10:57,848 - Epoch: 409, Steps: 235 | Lr: 2.33742e-05 | Train Loss: 2.18783e-04
2024-03-15 14:10:57,848 - Epoch time: 25.47s, Average time: 25.61s
2024-03-15 14:10:58,179 - 
2024-03-15 14:11:23,860 - Epoch: 410, Steps: 235 | Lr: 2.30152e-05 | Train Loss: 2.21523e-04
2024-03-15 14:11:23,860 - Epoch time: 25.68s, Average time: 25.61s
2024-03-15 14:11:24,180 - 
2024-03-15 14:11:49,853 - Epoch: 411, Steps: 235 | Lr: 2.26582e-05 | Train Loss: 2.15713e-04
2024-03-15 14:11:49,854 - Epoch time: 25.67s, Average time: 25.61s
2024-03-15 14:11:50,167 - 
2024-03-15 14:12:15,653 - Epoch: 412, Steps: 235 | Lr: 2.23031e-05 | Train Loss: 2.18134e-04
2024-03-15 14:12:15,654 - Epoch time: 25.49s, Average time: 25.61s
2024-03-15 14:12:15,964 - 
2024-03-15 14:12:41,658 - Epoch: 413, Steps: 235 | Lr: 2.19500e-05 | Train Loss: 2.19812e-04
2024-03-15 14:12:41,658 - Epoch time: 25.69s, Average time: 25.61s
2024-03-15 14:12:41,971 - 
2024-03-15 14:13:07,960 - Epoch: 414, Steps: 235 | Lr: 2.15990e-05 | Train Loss: 2.21083e-04
2024-03-15 14:13:07,960 - Epoch time: 25.99s, Average time: 25.62s
2024-03-15 14:13:08,273 - 
2024-03-15 14:13:33,838 - Epoch: 415, Steps: 235 | Lr: 2.12501e-05 | Train Loss: 2.18200e-04
2024-03-15 14:13:33,838 - Epoch time: 25.56s, Average time: 25.62s
2024-03-15 14:13:34,151 - 
2024-03-15 14:13:59,569 - Epoch: 416, Steps: 235 | Lr: 2.09032e-05 | Train Loss: 2.17886e-04
2024-03-15 14:13:59,569 - Epoch time: 25.42s, Average time: 25.61s
2024-03-15 14:13:59,888 - 
2024-03-15 14:14:25,403 - Epoch: 417, Steps: 235 | Lr: 2.05584e-05 | Train Loss: 2.19564e-04
2024-03-15 14:14:25,403 - Epoch time: 25.51s, Average time: 25.61s
2024-03-15 14:14:25,721 - 
2024-03-15 14:14:51,245 - Epoch: 418, Steps: 235 | Lr: 2.02157e-05 | Train Loss: 2.11607e-04
2024-03-15 14:14:51,246 - Epoch time: 25.52s, Average time: 25.61s
2024-03-15 14:14:51,246 - Loss decreased (2.14769e-04 --> 2.11607e-04).  Saving best model ...
2024-03-15 14:14:51,663 - 
2024-03-15 14:15:17,262 - Epoch: 419, Steps: 235 | Lr: 1.98752e-05 | Train Loss: 2.14576e-04
2024-03-15 14:15:17,262 - Epoch time: 25.60s, Average time: 25.61s
2024-03-15 14:15:17,570 - 
2024-03-15 14:15:43,346 - Epoch: 420, Steps: 235 | Lr: 1.95369e-05 | Train Loss: 2.12880e-04
2024-03-15 14:15:43,346 - Epoch time: 25.78s, Average time: 25.61s
2024-03-15 14:15:43,650 - 
2024-03-15 14:16:09,133 - Epoch: 421, Steps: 235 | Lr: 1.92008e-05 | Train Loss: 2.12310e-04
2024-03-15 14:16:09,133 - Epoch time: 25.48s, Average time: 25.61s
2024-03-15 14:16:09,439 - 
2024-03-15 14:16:34,959 - Epoch: 422, Steps: 235 | Lr: 1.88669e-05 | Train Loss: 2.06816e-04
2024-03-15 14:16:34,959 - Epoch time: 25.52s, Average time: 25.61s
2024-03-15 14:16:34,960 - Loss decreased (2.11607e-04 --> 2.06816e-04).  Saving best model ...
2024-03-15 14:16:35,380 - 
2024-03-15 14:17:00,784 - Epoch: 423, Steps: 235 | Lr: 1.85352e-05 | Train Loss: 2.14125e-04
2024-03-15 14:17:00,784 - Epoch time: 25.40s, Average time: 25.61s
2024-03-15 14:17:01,121 - 
2024-03-15 14:17:26,948 - Epoch: 424, Steps: 235 | Lr: 1.82059e-05 | Train Loss: 2.18382e-04
2024-03-15 14:17:26,949 - Epoch time: 25.83s, Average time: 25.61s
2024-03-15 14:17:27,287 - 
2024-03-15 14:17:53,172 - Epoch: 425, Steps: 235 | Lr: 1.78788e-05 | Train Loss: 2.17594e-04
2024-03-15 14:17:53,172 - Epoch time: 25.88s, Average time: 25.61s
2024-03-15 14:17:53,514 - 
2024-03-15 14:18:19,294 - Epoch: 426, Steps: 235 | Lr: 1.75541e-05 | Train Loss: 2.10616e-04
2024-03-15 14:18:19,294 - Epoch time: 25.78s, Average time: 25.61s
2024-03-15 14:18:19,623 - 
2024-03-15 14:18:45,374 - Epoch: 427, Steps: 235 | Lr: 1.72317e-05 | Train Loss: 2.10785e-04
2024-03-15 14:18:45,374 - Epoch time: 25.75s, Average time: 25.62s
2024-03-15 14:18:45,701 - 
2024-03-15 14:19:11,193 - Epoch: 428, Steps: 235 | Lr: 1.69117e-05 | Train Loss: 2.07630e-04
2024-03-15 14:19:11,193 - Epoch time: 25.49s, Average time: 25.61s
2024-03-15 14:19:11,505 - 
2024-03-15 14:19:37,213 - Epoch: 429, Steps: 235 | Lr: 1.65940e-05 | Train Loss: 2.10723e-04
2024-03-15 14:19:37,213 - Epoch time: 25.71s, Average time: 25.62s
2024-03-15 14:19:37,531 - 
2024-03-15 14:20:03,326 - Epoch: 430, Steps: 235 | Lr: 1.62788e-05 | Train Loss: 2.03590e-04
2024-03-15 14:20:03,327 - Epoch time: 25.80s, Average time: 25.62s
2024-03-15 14:20:03,327 - Loss decreased (2.06816e-04 --> 2.03590e-04).  Saving best model ...
2024-03-15 14:20:03,757 - 
2024-03-15 14:20:29,273 - Epoch: 431, Steps: 235 | Lr: 1.59661e-05 | Train Loss: 2.05513e-04
2024-03-15 14:20:29,273 - Epoch time: 25.52s, Average time: 25.62s
2024-03-15 14:20:29,604 - 
2024-03-15 14:20:55,355 - Epoch: 432, Steps: 235 | Lr: 1.56558e-05 | Train Loss: 2.03162e-04
2024-03-15 14:20:55,355 - Epoch time: 25.75s, Average time: 25.62s
2024-03-15 14:20:55,355 - Loss decreased (2.03590e-04 --> 2.03162e-04).  Saving best model ...
2024-03-15 14:20:55,788 - 
2024-03-15 14:21:21,692 - Epoch: 433, Steps: 235 | Lr: 1.53479e-05 | Train Loss: 2.07217e-04
2024-03-15 14:21:21,693 - Epoch time: 25.90s, Average time: 25.62s
2024-03-15 14:21:22,004 - 
2024-03-15 14:21:47,735 - Epoch: 434, Steps: 235 | Lr: 1.50426e-05 | Train Loss: 2.06076e-04
2024-03-15 14:21:47,735 - Epoch time: 25.73s, Average time: 25.62s
2024-03-15 14:21:48,055 - 
2024-03-15 14:22:13,419 - Epoch: 435, Steps: 235 | Lr: 1.47398e-05 | Train Loss: 2.06810e-04
2024-03-15 14:22:13,419 - Epoch time: 25.36s, Average time: 25.62s
2024-03-15 14:22:13,746 - 
2024-03-15 14:22:39,397 - Epoch: 436, Steps: 235 | Lr: 1.44396e-05 | Train Loss: 2.01914e-04
2024-03-15 14:22:39,397 - Epoch time: 25.65s, Average time: 25.62s
2024-03-15 14:22:39,398 - Loss decreased (2.03162e-04 --> 2.01914e-04).  Saving best model ...
2024-03-15 14:22:39,833 - 
2024-03-15 14:23:05,444 - Epoch: 437, Steps: 235 | Lr: 1.41419e-05 | Train Loss: 2.05443e-04
2024-03-15 14:23:05,445 - Epoch time: 25.61s, Average time: 25.62s
2024-03-15 14:23:05,769 - 
2024-03-15 14:23:31,193 - Epoch: 438, Steps: 235 | Lr: 1.38468e-05 | Train Loss: 2.00375e-04
2024-03-15 14:23:31,194 - Epoch time: 25.42s, Average time: 25.62s
2024-03-15 14:23:31,194 - Loss decreased (2.01914e-04 --> 2.00375e-04).  Saving best model ...
2024-03-15 14:23:31,621 - 
2024-03-15 14:23:57,038 - Epoch: 439, Steps: 235 | Lr: 1.35544e-05 | Train Loss: 1.95068e-04
2024-03-15 14:23:57,038 - Epoch time: 25.42s, Average time: 25.62s
2024-03-15 14:23:57,038 - Loss decreased (2.00375e-04 --> 1.95068e-04).  Saving best model ...
2024-03-15 14:23:57,461 - 
2024-03-15 14:24:23,132 - Epoch: 440, Steps: 235 | Lr: 1.32646e-05 | Train Loss: 2.02804e-04
2024-03-15 14:24:23,133 - Epoch time: 25.67s, Average time: 25.62s
2024-03-15 14:24:23,463 - 
2024-03-15 14:24:49,135 - Epoch: 441, Steps: 235 | Lr: 1.29774e-05 | Train Loss: 1.99426e-04
2024-03-15 14:24:49,136 - Epoch time: 25.67s, Average time: 25.62s
2024-03-15 14:24:49,473 - 
2024-03-15 14:25:15,118 - Epoch: 442, Steps: 235 | Lr: 1.26929e-05 | Train Loss: 2.03039e-04
2024-03-15 14:25:15,118 - Epoch time: 25.64s, Average time: 25.62s
2024-03-15 14:25:15,430 - 
2024-03-15 14:25:40,832 - Epoch: 443, Steps: 235 | Lr: 1.24111e-05 | Train Loss: 2.02134e-04
2024-03-15 14:25:40,832 - Epoch time: 25.40s, Average time: 25.62s
2024-03-15 14:25:41,141 - 
2024-03-15 14:26:06,537 - Epoch: 444, Steps: 235 | Lr: 1.21321e-05 | Train Loss: 2.02667e-04
2024-03-15 14:26:06,537 - Epoch time: 25.40s, Average time: 25.61s
2024-03-15 14:26:06,851 - 
2024-03-15 14:26:32,106 - Epoch: 445, Steps: 235 | Lr: 1.18558e-05 | Train Loss: 1.98861e-04
2024-03-15 14:26:32,106 - Epoch time: 25.25s, Average time: 25.61s
2024-03-15 14:26:32,423 - 
2024-03-15 14:26:57,699 - Epoch: 446, Steps: 235 | Lr: 1.15822e-05 | Train Loss: 1.97568e-04
2024-03-15 14:26:57,699 - Epoch time: 25.28s, Average time: 25.61s
2024-03-15 14:26:58,037 - 
2024-03-15 14:27:23,773 - Epoch: 447, Steps: 235 | Lr: 1.13114e-05 | Train Loss: 2.03849e-04
2024-03-15 14:27:23,773 - Epoch time: 25.74s, Average time: 25.61s
2024-03-15 14:27:24,085 - 
2024-03-15 14:27:49,771 - Epoch: 448, Steps: 235 | Lr: 1.10434e-05 | Train Loss: 2.02797e-04
2024-03-15 14:27:49,772 - Epoch time: 25.69s, Average time: 25.61s
2024-03-15 14:27:50,115 - 
2024-03-15 14:28:15,841 - Epoch: 449, Steps: 235 | Lr: 1.07782e-05 | Train Loss: 1.92308e-04
2024-03-15 14:28:15,842 - Epoch time: 25.73s, Average time: 25.61s
2024-03-15 14:28:15,842 - Loss decreased (1.95068e-04 --> 1.92308e-04).  Saving best model ...
2024-03-15 14:28:16,255 - 
2024-03-15 14:28:41,787 - Epoch: 450, Steps: 235 | Lr: 1.05159e-05 | Train Loss: 2.05899e-04
2024-03-15 14:28:41,787 - Epoch time: 25.53s, Average time: 25.61s
2024-03-15 14:28:41,791 - Save the loss_recoder of 450 epochs
2024-03-15 14:28:41,812 - Simulation picture saved in /home/bingxing2/home/scx6mfb/mhy/SAU/CFD-CNN/Model/SAU_1001_10to1-0.3/pic/loss_record.png
2024-03-15 14:28:42,811 - 
2024-03-15 14:29:08,236 - Epoch: 451, Steps: 235 | Lr: 1.02564e-05 | Train Loss: 1.97212e-04
2024-03-15 14:29:08,237 - Epoch time: 25.42s, Average time: 25.61s
2024-03-15 14:29:08,559 - 
2024-03-15 14:29:33,936 - Epoch: 452, Steps: 235 | Lr: 9.99983e-06 | Train Loss: 1.96376e-04
2024-03-15 14:29:33,936 - Epoch time: 25.38s, Average time: 25.61s
2024-03-15 14:29:34,253 - 
2024-03-15 14:29:59,859 - Epoch: 453, Steps: 235 | Lr: 9.74613e-06 | Train Loss: 1.97513e-04
2024-03-15 14:29:59,859 - Epoch time: 25.61s, Average time: 25.61s
2024-03-15 14:30:00,170 - 
2024-03-15 14:30:25,564 - Epoch: 454, Steps: 235 | Lr: 9.49533e-06 | Train Loss: 1.96165e-04
2024-03-15 14:30:25,565 - Epoch time: 25.39s, Average time: 25.61s
2024-03-15 14:30:25,895 - 
2024-03-15 14:30:51,470 - Epoch: 455, Steps: 235 | Lr: 9.24747e-06 | Train Loss: 1.95142e-04
2024-03-15 14:30:51,470 - Epoch time: 25.57s, Average time: 25.61s
2024-03-15 14:30:51,786 - 
2024-03-15 14:31:17,420 - Epoch: 456, Steps: 235 | Lr: 9.00255e-06 | Train Loss: 1.97038e-04
2024-03-15 14:31:17,420 - Epoch time: 25.63s, Average time: 25.61s
2024-03-15 14:31:17,740 - 
2024-03-15 14:31:43,590 - Epoch: 457, Steps: 235 | Lr: 8.76060e-06 | Train Loss: 1.93349e-04
2024-03-15 14:31:43,590 - Epoch time: 25.85s, Average time: 25.61s
2024-03-15 14:31:43,924 - 
2024-03-15 14:32:09,489 - Epoch: 458, Steps: 235 | Lr: 8.52163e-06 | Train Loss: 1.93973e-04
2024-03-15 14:32:09,490 - Epoch time: 25.57s, Average time: 25.61s
2024-03-15 14:32:09,812 - 
2024-03-15 14:32:35,147 - Epoch: 459, Steps: 235 | Lr: 8.28567e-06 | Train Loss: 1.99052e-04
2024-03-15 14:32:35,147 - Epoch time: 25.33s, Average time: 25.61s
2024-03-15 14:32:35,462 - 
2024-03-15 14:33:01,031 - Epoch: 460, Steps: 235 | Lr: 8.05272e-06 | Train Loss: 1.97253e-04
2024-03-15 14:33:01,031 - Epoch time: 25.57s, Average time: 25.61s
2024-03-15 14:33:01,365 - 
2024-03-15 14:33:26,818 - Epoch: 461, Steps: 235 | Lr: 7.82280e-06 | Train Loss: 1.96850e-04
2024-03-15 14:33:26,818 - Epoch time: 25.45s, Average time: 25.61s
2024-03-15 14:33:27,149 - 
2024-03-15 14:33:52,792 - Epoch: 462, Steps: 235 | Lr: 7.59594e-06 | Train Loss: 1.97087e-04
2024-03-15 14:33:52,792 - Epoch time: 25.64s, Average time: 25.61s
2024-03-15 14:33:53,106 - 
2024-03-15 14:34:18,719 - Epoch: 463, Steps: 235 | Lr: 7.37214e-06 | Train Loss: 1.94633e-04
2024-03-15 14:34:18,719 - Epoch time: 25.61s, Average time: 25.61s
2024-03-15 14:34:19,030 - 
2024-03-15 14:34:44,532 - Epoch: 464, Steps: 235 | Lr: 7.15143e-06 | Train Loss: 1.96627e-04
2024-03-15 14:34:44,532 - Epoch time: 25.50s, Average time: 25.61s
2024-03-15 14:34:44,856 - 
2024-03-15 14:35:10,482 - Epoch: 465, Steps: 235 | Lr: 6.93382e-06 | Train Loss: 1.90129e-04
2024-03-15 14:35:10,482 - Epoch time: 25.63s, Average time: 25.61s
2024-03-15 14:35:10,483 - Loss decreased (1.92308e-04 --> 1.90129e-04).  Saving best model ...
2024-03-15 14:35:10,904 - 
2024-03-15 14:35:36,179 - Epoch: 466, Steps: 235 | Lr: 6.71933e-06 | Train Loss: 1.89465e-04
2024-03-15 14:35:36,180 - Epoch time: 25.28s, Average time: 25.61s
2024-03-15 14:35:36,180 - Loss decreased (1.90129e-04 --> 1.89465e-04).  Saving best model ...
2024-03-15 14:35:36,594 - 
2024-03-15 14:36:02,272 - Epoch: 467, Steps: 235 | Lr: 6.50796e-06 | Train Loss: 1.92451e-04
2024-03-15 14:36:02,272 - Epoch time: 25.68s, Average time: 25.61s
2024-03-15 14:36:02,603 - 
2024-03-15 14:36:28,002 - Epoch: 468, Steps: 235 | Lr: 6.29974e-06 | Train Loss: 1.90751e-04
2024-03-15 14:36:28,003 - Epoch time: 25.40s, Average time: 25.61s
2024-03-15 14:36:28,325 - 
2024-03-15 14:36:53,920 - Epoch: 469, Steps: 235 | Lr: 6.09468e-06 | Train Loss: 1.88783e-04
2024-03-15 14:36:53,920 - Epoch time: 25.59s, Average time: 25.61s
2024-03-15 14:36:53,920 - Loss decreased (1.89465e-04 --> 1.88783e-04).  Saving best model ...
2024-03-15 14:36:54,344 - 
2024-03-15 14:37:19,948 - Epoch: 470, Steps: 235 | Lr: 5.89280e-06 | Train Loss: 1.93794e-04
2024-03-15 14:37:19,948 - Epoch time: 25.60s, Average time: 25.61s
2024-03-15 14:37:20,262 - 
2024-03-15 14:37:46,035 - Epoch: 471, Steps: 235 | Lr: 5.69411e-06 | Train Loss: 1.85870e-04
2024-03-15 14:37:46,036 - Epoch time: 25.77s, Average time: 25.61s
2024-03-15 14:37:46,036 - Loss decreased (1.88783e-04 --> 1.85870e-04).  Saving best model ...
2024-03-15 14:37:46,454 - 
2024-03-15 14:38:12,044 - Epoch: 472, Steps: 235 | Lr: 5.49863e-06 | Train Loss: 1.88224e-04
2024-03-15 14:38:12,044 - Epoch time: 25.59s, Average time: 25.61s
2024-03-15 14:38:12,383 - 
2024-03-15 14:38:37,938 - Epoch: 473, Steps: 235 | Lr: 5.30636e-06 | Train Loss: 1.95076e-04
2024-03-15 14:38:37,938 - Epoch time: 25.55s, Average time: 25.61s
2024-03-15 14:38:38,259 - 
2024-03-15 14:39:03,711 - Epoch: 474, Steps: 235 | Lr: 5.11733e-06 | Train Loss: 1.94964e-04
2024-03-15 14:39:03,711 - Epoch time: 25.45s, Average time: 25.61s
2024-03-15 14:39:04,022 - 
2024-03-15 14:39:29,572 - Epoch: 475, Steps: 235 | Lr: 4.93154e-06 | Train Loss: 1.92385e-04
2024-03-15 14:39:29,572 - Epoch time: 25.55s, Average time: 25.61s
2024-03-15 14:39:29,888 - 
2024-03-15 14:39:55,543 - Epoch: 476, Steps: 235 | Lr: 4.74902e-06 | Train Loss: 1.88837e-04
2024-03-15 14:39:55,543 - Epoch time: 25.65s, Average time: 25.61s
2024-03-15 14:39:55,858 - 
2024-03-15 14:40:21,205 - Epoch: 477, Steps: 235 | Lr: 4.56976e-06 | Train Loss: 1.91570e-04
2024-03-15 14:40:21,206 - Epoch time: 25.35s, Average time: 25.61s
2024-03-15 14:40:21,530 - 
2024-03-15 14:40:47,300 - Epoch: 478, Steps: 235 | Lr: 4.39380e-06 | Train Loss: 1.94730e-04
2024-03-15 14:40:47,300 - Epoch time: 25.77s, Average time: 25.61s
2024-03-15 14:40:47,616 - 
2024-03-15 14:41:13,060 - Epoch: 479, Steps: 235 | Lr: 4.22113e-06 | Train Loss: 1.87061e-04
2024-03-15 14:41:13,060 - Epoch time: 25.44s, Average time: 25.61s
2024-03-15 14:41:13,378 - 
2024-03-15 14:41:38,986 - Epoch: 480, Steps: 235 | Lr: 4.05178e-06 | Train Loss: 1.91135e-04
2024-03-15 14:41:38,986 - Epoch time: 25.61s, Average time: 25.61s
2024-03-15 14:41:39,307 - 
2024-03-15 14:42:05,128 - Epoch: 481, Steps: 235 | Lr: 3.88575e-06 | Train Loss: 1.87237e-04
2024-03-15 14:42:05,128 - Epoch time: 25.82s, Average time: 25.61s
2024-03-15 14:42:05,449 - 
2024-03-15 14:42:31,170 - Epoch: 482, Steps: 235 | Lr: 3.72306e-06 | Train Loss: 1.92228e-04
2024-03-15 14:42:31,171 - Epoch time: 25.72s, Average time: 25.61s
2024-03-15 14:42:31,500 - 
2024-03-15 14:42:57,242 - Epoch: 483, Steps: 235 | Lr: 3.56371e-06 | Train Loss: 1.88916e-04
2024-03-15 14:42:57,242 - Epoch time: 25.74s, Average time: 25.61s
2024-03-15 14:42:57,576 - 
2024-03-15 14:43:23,101 - Epoch: 484, Steps: 235 | Lr: 3.40772e-06 | Train Loss: 1.85251e-04
2024-03-15 14:43:23,101 - Epoch time: 25.52s, Average time: 25.61s
2024-03-15 14:43:23,101 - Loss decreased (1.85870e-04 --> 1.85251e-04).  Saving best model ...
2024-03-15 14:43:23,527 - 
2024-03-15 14:43:48,944 - Epoch: 485, Steps: 235 | Lr: 3.25511e-06 | Train Loss: 1.84573e-04
2024-03-15 14:43:48,944 - Epoch time: 25.42s, Average time: 25.61s
2024-03-15 14:43:48,945 - Loss decreased (1.85251e-04 --> 1.84573e-04).  Saving best model ...
2024-03-15 14:43:49,370 - 
2024-03-15 14:44:15,120 - Epoch: 486, Steps: 235 | Lr: 3.10587e-06 | Train Loss: 1.87342e-04
2024-03-15 14:44:15,121 - Epoch time: 25.75s, Average time: 25.61s
2024-03-15 14:44:15,430 - 
2024-03-15 14:44:41,019 - Epoch: 487, Steps: 235 | Lr: 2.96003e-06 | Train Loss: 1.91005e-04
2024-03-15 14:44:41,019 - Epoch time: 25.59s, Average time: 25.61s
2024-03-15 14:44:41,350 - 
2024-03-15 14:45:07,140 - Epoch: 488, Steps: 235 | Lr: 2.81759e-06 | Train Loss: 1.89078e-04
2024-03-15 14:45:07,140 - Epoch time: 25.79s, Average time: 25.61s
2024-03-15 14:45:07,467 - 
2024-03-15 14:45:33,279 - Epoch: 489, Steps: 235 | Lr: 2.67856e-06 | Train Loss: 1.89808e-04
2024-03-15 14:45:33,280 - Epoch time: 25.81s, Average time: 25.61s
2024-03-15 14:45:33,592 - 
2024-03-15 14:45:59,412 - Epoch: 490, Steps: 235 | Lr: 2.54296e-06 | Train Loss: 1.86756e-04
2024-03-15 14:45:59,413 - Epoch time: 25.82s, Average time: 25.61s
2024-03-15 14:45:59,729 - 
2024-03-15 14:46:25,363 - Epoch: 491, Steps: 235 | Lr: 2.41079e-06 | Train Loss: 1.89033e-04
2024-03-15 14:46:25,364 - Epoch time: 25.63s, Average time: 25.61s
2024-03-15 14:46:25,678 - 
2024-03-15 14:46:51,252 - Epoch: 492, Steps: 235 | Lr: 2.28207e-06 | Train Loss: 1.93243e-04
2024-03-15 14:46:51,252 - Epoch time: 25.57s, Average time: 25.61s
2024-03-15 14:46:51,566 - 
2024-03-15 14:47:17,026 - Epoch: 493, Steps: 235 | Lr: 2.15679e-06 | Train Loss: 1.88970e-04
2024-03-15 14:47:17,026 - Epoch time: 25.46s, Average time: 25.61s
2024-03-15 14:47:17,348 - 
2024-03-15 14:47:42,863 - Epoch: 494, Steps: 235 | Lr: 2.03498e-06 | Train Loss: 1.92620e-04
2024-03-15 14:47:42,863 - Epoch time: 25.52s, Average time: 25.61s
2024-03-15 14:47:43,200 - 
2024-03-15 14:48:08,889 - Epoch: 495, Steps: 235 | Lr: 1.91663e-06 | Train Loss: 1.89671e-04
2024-03-15 14:48:08,889 - Epoch time: 25.69s, Average time: 25.61s
2024-03-15 14:48:09,199 - 
2024-03-15 14:48:34,681 - Epoch: 496, Steps: 235 | Lr: 1.80177e-06 | Train Loss: 1.87637e-04
2024-03-15 14:48:34,682 - Epoch time: 25.48s, Average time: 25.61s
2024-03-15 14:48:34,992 - 
2024-03-15 14:49:00,489 - Epoch: 497, Steps: 235 | Lr: 1.69039e-06 | Train Loss: 1.87039e-04
2024-03-15 14:49:00,489 - Epoch time: 25.50s, Average time: 25.61s
2024-03-15 14:49:00,808 - 
2024-03-15 14:49:26,219 - Epoch: 498, Steps: 235 | Lr: 1.58251e-06 | Train Loss: 1.83141e-04
2024-03-15 14:49:26,219 - Epoch time: 25.41s, Average time: 25.61s
2024-03-15 14:49:26,220 - Loss decreased (1.84573e-04 --> 1.83141e-04).  Saving best model ...
2024-03-15 14:49:26,642 - 
2024-03-15 14:49:51,887 - Epoch: 499, Steps: 235 | Lr: 1.47813e-06 | Train Loss: 1.89301e-04
2024-03-15 14:49:51,887 - Epoch time: 25.24s, Average time: 25.61s
2024-03-15 14:49:52,196 - 
2024-03-15 14:50:17,735 - Epoch: 500, Steps: 235 | Lr: 1.37726e-06 | Train Loss: 1.90120e-04
2024-03-15 14:50:17,735 - Epoch time: 25.54s, Average time: 25.61s
2024-03-15 14:50:17,740 - Save the loss_recoder of 500 epochs
2024-03-15 14:50:17,761 - Simulation picture saved in /home/bingxing2/home/scx6mfb/mhy/SAU/CFD-CNN/Model/SAU_1001_10to1-0.3/pic/loss_record.png
2024-03-15 14:50:18,754 - 
2024-03-15 14:50:19,991 - Successful load model state_dict
2024-03-15 14:50:20,959 - Computed:
mse_T : 1.79720e-04, mse   : 1.79720e-04
mae_T : 3.92494e-03, mae   : 3.92494e-03
rmse_T: 1.34060e-02, rmse  : 1.34060e-02
mre_T : 5.84014e-02, mre   : 5.84014e-02
mxre_T: 7.66891e+03, mxre  : 7.66891e+03
ssim_T: 9.98594e-01, ssim  : 9.98594e-01

2024-03-15 14:50:26,211 - after: 0, Computed picture is saved
2024-03-15 14:50:26,702 - 
Original:
mse_T : 9.43074e-04, mse   : 9.43074e-04
mae_T : 8.99098e-03, mae   : 8.99098e-03
rmse_T: 3.07095e-02, rmse  : 3.07095e-02
mre_T : 3.00261e-03, mre   : 3.00261e-03
mxre_T: 9.94967e-01, mxre  : 9.94967e-01
ssim_T: 9.99019e-01, ssim  : 9.99019e-01

2024-03-15 14:50:31,747 - after: 0, Original picture is saved
